{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 1875 train samples, and 313 test samples\n"
     ]
    }
   ],
   "source": [
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "mnist_ds_path = '../datasets'\n",
    "# you can change the batch_size, epochs, batch_limited to reach different accuracy\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "batch_limited = 10000\n",
    "alpha = 0.1\n",
    "classes_num = 10\n",
    "hidden_size = 128\n",
    "w, h = 28, 28\n",
    "layer0_w = np.random.normal(0, 0.01, (w*h, hidden_size))\n",
    "layer1_w = np.random.normal(0, 0.01, (hidden_size, classes_num))\n",
    "\n",
    "train_ds = tv.datasets.MNIST(root=mnist_ds_path, train=True, transform=transforms.ToTensor())\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)  #60000\n",
    "\n",
    "test_ds = tv.datasets.MNIST(root=mnist_ds_path, train=False, transform=transforms.ToTensor())\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)    #10000\n",
    "\n",
    "print('we have %d train samples, and %d test samples' % (len(train_dataloader), len(test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.where(x>=0, x, 0)\n",
    "\n",
    "def relu2driv(x):\n",
    "    return np.where(x>0, 1, 0)\n",
    "\n",
    "def dropout(x, dropout_prob):\n",
    "    assert dropout_prob>=0 and dropout_prob<1   # when dropout_prob == 1, all are dropped out; dropout_prob == 0, nothing is dropped out\n",
    "    \n",
    "    ratio = 1.0/dropout_prob\n",
    "    \n",
    "    mask = np.random.uniform(0, 1.0, size=x.shape)\n",
    "    mask = (mask >= dropout_prob).astype(np.int32)\n",
    "    \n",
    "    return x*mask*ratio, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(gt, pred):\n",
    "    num = gt.shape[0]\n",
    "    y = np.zeros((num, classes_num))\n",
    "    y[np.arange(num), gt] = 1\n",
    "    return np.sum((y-pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(gt, pred):\n",
    "    pred = pred.argmax(axis=1)\n",
    "    return np.mean(gt == pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, is_training, dropout_prob=0.5):\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    mid_pred = relu(x.dot(layer0_w))\n",
    "    \n",
    "    if is_training and dropout_prob>0:\n",
    "        mask_pred, mask = dropout(mid_pred, dropout_prob)\n",
    "    else:\n",
    "        mask_pred = mid_pred\n",
    "        mask = None\n",
    "\n",
    "    pred = mask_pred.dot(layer1_w)\n",
    "    \n",
    "    return pred, mid_pred, mask\n",
    "\n",
    "def backward(gt, pred, mid_pred, mask, x):\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    num = gt.shape[0]\n",
    "    y = np.zeros((num, classes_num))\n",
    "    y[np.arange(num), gt] = 1\n",
    "    \n",
    "    delta = 2*(pred-y)/num\n",
    "    layer1_w_step = mid_pred.T.dot(delta)\n",
    "    \n",
    "    w_delta = delta.dot(layer1_w.T)*relu2driv(mid_pred)\n",
    "    if mask is not None:\n",
    "        w_delta = w_delta*mask\n",
    "    layer0_w_step = x.T.dot(w_delta)\n",
    "    \n",
    "    return layer0_w_step, layer1_w_step\n",
    "\n",
    "def step(alpha, layer0_w_step, layer1_w_step):\n",
    "    global layer0_w, layer1_w\n",
    "    layer0_w = layer0_w - alpha * layer0_w_step\n",
    "    layer1_w = layer1_w - alpha * layer1_w_step\n",
    "    \n",
    "def test():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for x,y in test_dataloader:\n",
    "        x = x.cpu().numpy()\n",
    "        y = y.cpu().numpy()\n",
    "        \n",
    "        pred, _, _ = forward(x, False)\n",
    "        loss += mse(y, pred)\n",
    "        accuracy += acc(y, pred)\n",
    "    \n",
    "    return loss/len(test_dataloader), accuracy/len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in epoch 0, train loss: 6.5584, train acc: 0.9151, test loss: 4.2852, test acc: 0.9583\n",
      "in epoch 1, train loss: 3.9856, train acc: 0.9621, test loss: 3.6649, test acc: 0.9654\n",
      "in epoch 2, train loss: 3.4526, train acc: 0.9697, test loss: 3.4377, test acc: 0.9689\n",
      "in epoch 3, train loss: 3.1463, train acc: 0.9744, test loss: 3.2547, test acc: 0.9709\n",
      "in epoch 4, train loss: 2.9640, train acc: 0.9767, test loss: 3.1057, test acc: 0.9726\n",
      "in epoch 5, train loss: 2.8228, train acc: 0.9779, test loss: 2.9561, test acc: 0.9727\n",
      "in epoch 6, train loss: 2.7194, train acc: 0.9790, test loss: 2.9647, test acc: 0.9719\n",
      "in epoch 7, train loss: 2.6182, train acc: 0.9806, test loss: 2.9458, test acc: 0.9730\n",
      "in epoch 8, train loss: 2.5489, train acc: 0.9812, test loss: 2.8696, test acc: 0.9738\n",
      "in epoch 9, train loss: 2.4796, train acc: 0.9818, test loss: 2.8495, test acc: 0.9739\n",
      "in epoch 10, train loss: 2.4245, train acc: 0.9825, test loss: 2.7945, test acc: 0.9753\n",
      "in epoch 11, train loss: 2.3737, train acc: 0.9833, test loss: 2.7049, test acc: 0.9743\n",
      "in epoch 12, train loss: 2.3284, train acc: 0.9839, test loss: 2.9093, test acc: 0.9739\n",
      "in epoch 13, train loss: 2.2888, train acc: 0.9845, test loss: 2.7409, test acc: 0.9762\n",
      "in epoch 14, train loss: 2.2540, train acc: 0.9850, test loss: 2.7582, test acc: 0.9753\n"
     ]
    }
   ],
   "source": [
    "dropout_prob = 0.0\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for idx, (x,y) in enumerate(train_dataloader):\n",
    "        x = x.cpu().numpy()\n",
    "        y = y.cpu().numpy()\n",
    "        \n",
    "        pred, mid_pred, mask = forward(x, True, dropout_prob)\n",
    "        loss += mse(y, pred)\n",
    "        accuracy += acc(y, pred)\n",
    "        \n",
    "        layer0_w_step, layer1_w_step = backward(y, pred, mid_pred, mask, x)\n",
    "        step(alpha, layer0_w_step, layer1_w_step)\n",
    "        \n",
    "        if idx >= batch_limited-1:\n",
    "            break\n",
    "    \n",
    "    # do test\n",
    "    test_loss, test_acc = test()\n",
    "    print('in epoch %d, train loss: %.4f, train acc: %.4f, test loss: %.4f, test acc: %.4f' % \\\n",
    "          (epoch, loss/(idx+1), accuracy/(idx+1), test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAFNklEQVR4nO3dsWoUaxzG4VkJYgKLIKJBSFIISaFYqJWNF5A2iJfgBYR0AbG1FLWwsU0jaGmxIAqCqIhVLIOFglgFFAUzVqc4kPkPZxNP3jXPU+blW6f5ZcGP3Qzatm2APEcO+gGA3YkTQokTQokTQokTQk1V42Aw8F+58Ie1bTvY7efeOSGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCGUOCHU1EE/wN9ocXGx3C9cuNC5nTlzpjx76dKlsZ7pHw8ePCj3Fy9e7On12T/eOSGUOCGUOCGUOCGUOCGUOCGUOCHUoG3b7nEw6B7pdP/+/XK/cePG2K/96dOncj958mS5f/nypdyXl5c7t3fv3pVnGU/btoPdfu6dE0KJE0KJE0KJE0KJE0KJE0KJE0K55/wDTpw4Ue6zs7Od2+nTp8uzr169Gvu1m6ZpRqNRuQ+Hw85taWmpPNt3h8ru3HPChBEnhBInhBInhBInhBInhHKVcsisrq6W++3btzu369evl2c3NjbGeqb/w9RU/S2wx48fL/eVlZXO7ezZs+XZtbW1cneVAhNGnBBKnBBKnBBKnBBKnBBKnBDKnwD8y8zMzJT7uXPnxn7tHz9+jH22aZpmfn6+3Kenpzu3ixcvlmevXr1a7leuXCn3hYWFcv/w4UPnduvWrfLsuLxzQihxQihxQihxQihxQihxQihxQqhD+XnO6j6tafr/RF/fn8J7//59uX/9+rXcK3Nzc+V+/vz5cn/06FG5Hzt2rHPb2toqz/78+bPc+569+rf36vHjx+V+7969cn/69Ol+Ps6/+DwnTBhxQihxQihxQihxQihxQihxQqhDec958+bNcl9fXy/3I0fq32l9n3v89etXuVf6vn/16NGjY792n+3t7XJ//fp1uVefiWyapnn27Fnn9vHjx/Js393z9+/fy31nZ6fc/yT3nDBhxAmhxAmhxAmhxAmhxAmhxAmhDuX31i4tLZV73z3m58+fy73vvq/6jtXhcFie7fPt27dyv3v3brk/efKkc3vz5k15tu8ukf/GOyeEEieEEieEEieEEieEEieEOpQfGbt8+XK5j0ajct/rdcdevH37ttzv3LlT7g8fPtzHp2E/+MgYTBhxQihxQihxQihxQihxQihxQqhDec/Z59SpU+V+7dq1cu+7R93c3Ozcnj9/Xp59+fJluR/kVzwyHvecMGHECaHECaHECaHECaHECaHECaHcc8IBc88JE0acEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEEqcEGrQtu1BPwOwC++cEEqcEEqcEEqcEEqcEEqcEOo3gKnPm67opi4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "im = x[0,0,:,:]\n",
    "fig = plt.figure()\n",
    "plotwindow = fig.add_subplot(111)\n",
    "plt.axis('off')\n",
    "plt.imshow(im, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 求导公式 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## １、模型末端使用sigmoid作为输出，并使用cross entropy作为Loss Function的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid 原始公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y=\\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid 求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y^{'}=-1\\frac{1}{(1+e^{-x})^2}e^{-x}(-1) =\\frac{e^{-x}}{(1+e^{-x})^2}=\\frac{1}{1+e^{-x}}\\frac{e^{-x}+1-1}{1+e^{-x}}=\\frac{1}{1+e^{-x}}(1-\\frac{1}{1+e^{-x}})=p_i(1-p_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross entropy 原始公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Loss=-y_ilogp_i-(1-y_i)log(1-p_i) $，   其中$y_i\\in(0,1)$，这里$y_i$是真实值，$p_i$是模型预测值，也就是模型最后一层输出＋sigmoid的结果，这里注意一下和下面softmax的区别，因为sigmoid的输出只有１个，一般根据训练的batch大小为Ｎ×１，所以Loss函数如上所示，需要判断$ y_i $是０还是１，但是softmax输出有ｍ个（m为类别个数），一个批次的输出为Ｎ×ｍ，所以此时Loss函数如下:$ Loss={\\sum_{j=0}^{n}{-y_ilogp_i}} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross entropy 求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "Loss^{'}=\n",
    "\\begin{cases}\n",
    "-1/p_i,&y_i=１ \\\\ \n",
    "1/(1-p_i),&y_i=0\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid+cross entropy　合在一起为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\begin{eqnarray*}\n",
    "\\frac{dLoss}{dx} &=& \\frac{dLoss}{dp_i}\\frac{dpi}{dx}\\\\\n",
    "&=& \\begin{cases}\n",
    "-1/p_i*p_i(1-p_i),&y_i=１ \\\\ \n",
    "1/(1-p_i)*p_i(1-p_i),&y_i=0\n",
    "\\end{cases} \\\\\n",
    "&=& \\begin{cases}\n",
    "p_i-1,&y_i=１ \\\\ \n",
    "p_i,&y_i=0\n",
    "\\end{cases} \\\\\n",
    "&=& p_i-y_i,&(y_i=0,y_i=1)\n",
    "\\end{eqnarray*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进一步的，上面求导公式中的ｘ为模型最后一层的输出，常见的情况是最后一层往往是一个Linear Classifier，比如pytorch常见的　　\n",
    "nn.Linear(input_channels, classes_num)，这里classes_num为预测的类型数量，我们设　$ x = X_{input}W $, 那么如  \n",
    "果要根据某一次前向推测的Loss计算$ W $的梯度，则我们只需要计算$ \\frac{dx}{dW} $即可，那么根据链式求导法则，我们有："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\begin{eqnarray*}\n",
    "\\frac{dLoss}{dW} &=& \\frac{dLoss}{dp_i}\\frac{dpi}{dx}\\frac{dx}{dＷ}\\\\\n",
    "&=& X_{input}.T*(p_i-y_i)\n",
    "\\end{eqnarray*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ２、模型末端使用softmax作为输出，并使用cross entropy作为Loss Function的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax原始公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ p_i = \\frac{e^{x_i}}{\\sum_{j=0}^{n}{e^{x_j}}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当$ i=j $时，$ \\begin{eqnarray*}\n",
    "p_i^{'} &=& \\frac{e^{x_i}}{\\sum_{j=0}^{n}{e^{x_j}}}+(-1)\\frac{e^{x_i}}{(\\sum_{j=0}^{n}{e^{x_j}})^2}e^{x_j}\\\\\n",
    "&=& \\frac{e^{x_i}}{\\sum_{j=0}^{n}{e^{x_j}}}-(\\frac{e^{x_i}}{\\sum_{j=0}^{n}{e^{x_j}}})^2\\\\\n",
    "&=& p_i-p_i^2\\\\\n",
    "&=& p_i(1-p_i)\n",
    "\\end{eqnarray*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当$ i \\neq j $时，$ \\begin{eqnarray*}\n",
    "p_i^{'} &=& (-1)\\frac{e^{x_i}}{(\\sum_{j=0}^{n}{e^{x_j}})^2}e^{x_j}\\\\\n",
    "&=& (-1)\\frac{e^{x_i}}{\\sum_{j=0}^{n}{e^j}}\\frac{e^{x_j}}{\\sum_{j=0}^{n}{e^{x_j}}}\\\\\n",
    "&=& -p_ip_j\n",
    "\\end{eqnarray*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以得到：$ \\frac{dp}{dx_i}=\n",
    "\\begin{cases}\n",
    "p_i(1-p_i),&i=j \\\\ \n",
    "-p_ip_j,&i \\neq j\n",
    "\\end{cases} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时交叉熵损失函数为$ Loss={\\sum_{j=0}^{n}{-y_ilogp_i}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其导数为  $ dLoss/dp_i = -1/p_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，$ dLoss/dx_i = \\frac{dLoss}{dp_i}\\frac{dp_i}{dx_i} = \\begin{cases}\n",
    "p_i(1-p_i)*-1/p_i=p_i-1,&i=j \\\\ \n",
    "-p_ip_j*-1/p_i=p_j,&i \\neq j\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们也可以统一到$ dLoss/dx_i = p-y $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y为one-hot编码矩阵，例如　$ \\left[ \\begin{array} {cccc}\n",
    "０&０&１&０\\\\\n",
    "０&０&１&０\\\\\n",
    "０&０&０&１\\\\\n",
    "１&０&０&０\n",
    "\\end{array} \\right] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p为模型最后层输出后的softmax结果，其shape与ｙ相同，都是N*m，N为本批次样本数，ｍ为类别数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('venv': venv)",
   "name": "ddea0ac9-f52a-4cac-bee8-4b6d21a58225"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}