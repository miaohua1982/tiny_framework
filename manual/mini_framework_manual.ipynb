{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8106/3593848235.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    def __init__(self, data, creator=None, create_op=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creator = creator\n",
    "        self.create_op = create_op\n",
    "        self.grad = None\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        return Tensor(self.data+other.data, (self, other), 'add')\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "    \n",
    "        if self.create_op == 'add':\n",
    "            self.creator[0].backward(self.grad)\n",
    "            self.creator[1].backward(self.grad)\n",
    "            \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a grad: [1 1 1 1 1]\n",
      "b grad: [2 2 2 2 2]\n",
      "c grad: [1 1 1 1 1]\n",
      "d grad: [1 1 1 1 1]\n",
      "e grad: [1 1 1 1 1]\n",
      "g grad: [3 3 3 3 3]\n",
      "h grad: [3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5])\n",
    "b = Tensor([2,2,2,2,2])\n",
    "c = Tensor([5,4,3,2,1])\n",
    "g = Tensor([1,2,3,4,5])\n",
    "h = Tensor([1,2,3,4,5])\n",
    "b = g + h\n",
    "d = a + b\n",
    "e = b + c\n",
    "f = d + e\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "print('a grad:', a.grad.data)\n",
    "print('b grad:', b.grad.data)\n",
    "print('c grad:', c.grad.data)\n",
    "print('d grad:', d.grad.data)\n",
    "print('e grad:', e.grad.data)\n",
    "print('g grad:', g.grad.data)  # g & h 's grad is error, so here we have to check grad for b\n",
    "print('h grad:', h.grad.data)  # b could backward only until all of b's previous nodes have be finished,\n",
    "                               # Let's check next version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4678998728"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(h.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Tensor(object):\n",
    "    def __init__(self, data, autograd=False, creator=None, create_op=None, id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.shape = self.data.shape\n",
    "        self.creator = creator\n",
    "        self.create_op = create_op\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        self.children = {}\n",
    "        if id is None:\n",
    "            id = np.random.choice(10000)\n",
    "        self.id = id\n",
    "        \n",
    "        if creator is not None:\n",
    "            for c in creator:\n",
    "                if self.id not in c.children:\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data+other.data, autograd=True, creator=(self, other), create_op='add')\n",
    "        return Tensor(self.data+other.data)\n",
    "        \n",
    "    def __neg__(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data*-1, autograd=True, creator=(self,), create_op='neg')\n",
    "        return Tensor(self.data*-1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data-other.data, autograd=True, creator=(self, other), create_op='sub')\n",
    "        return Tensor(self.data-other.data)\n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data*other.data, autograd=True, creator=(self, other), create_op='mul')\n",
    "        return Tensor(self.data*other.data)\n",
    "        \n",
    "    def sum(self, dim):\n",
    "        assert self.data.ndim>dim, 'axis %d is out of bounds for array of dimension %d' % (dim, self.data.ndim)\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.sum(dim), autograd=True, creator=(self,), create_op='sum_'+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "        \n",
    "    def expand(self, dim, copies):\n",
    "        assert self.data.ndim>=dim, 'axis %d is out of bounds for array of dimension %d' % (dim, self.data.ndim)\n",
    "        if self.autograd:\n",
    "            return Tensor(np.expand_dims(self.data, axis=dim).repeat(copies, axis=dim), autograd=True, creator=(self,), create_op='expand_'+str(dim))\n",
    "        return Tensor(np.expand_dims(self.data, axis=dim).repeat(copies, axis=dim))\n",
    "    \n",
    "    def transpose(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.transpose(), autograd=True, creator=(self,), create_op='transpose')\n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, other):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.dot(other.data), autograd=True, creator=(self, other), create_op='mm')\n",
    "        return Tensor(self.data.dot(other.data))\n",
    "    \n",
    "    def check_creator_grad_count(self):\n",
    "        for c in self.children:\n",
    "            if self.children[c] != 0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def backward(self, grad=None, child_grad_node=None):\n",
    "        if not self.autograd:\n",
    "            return\n",
    "        \n",
    "        if child_grad_node is not None:\n",
    "            if self.children[child_grad_node.id] == 0:\n",
    "                assert self.children[child_grad_node.id] != 0, \\\n",
    "                'creator %d'' children %d has grad count == 0, backprop can has one pass' % (self.id, child_grad_node.id)\n",
    "            else:\n",
    "                self.children[child_grad_node.id] -= 1\n",
    "\n",
    "        if grad is None:\n",
    "            grad = Tensor(np.ones_like(self.data, dtype=np.float, shape=self.shape))\n",
    "            \n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "   \n",
    "        if self.creator is not None and self.check_creator_grad_count():\n",
    "            if self.create_op == 'add':\n",
    "                self.creator[0].backward(self.grad, self)\n",
    "                self.creator[1].backward(self.grad, self)\n",
    "            elif self.create_op == 'neg':\n",
    "                self.creator[0].backward(self.grad.__neg__(), self)\n",
    "            elif self.create_op == 'sub':\n",
    "                self.creator[0].backward(self.grad, self)\n",
    "                self.creator[1].backward(self.grad.__neg__(), self)\n",
    "            elif self.create_op == 'mul':\n",
    "                self.creator[0].backward(self.grad*self.creator[1], self)\n",
    "                self.creator[1].backward(self.grad*self.creator[0], self)\n",
    "            elif self.create_op == 'transpose':\n",
    "                self.creator[0].backward(self.grad.transpose(), self)\n",
    "            elif self.create_op == 'mm':\n",
    "                c0 = Tensor(self.creator[0].data, autograd=False)  # no auto grad\n",
    "                c1 = Tensor(self.creator[1].data, autograd=False)  # no auto grad\n",
    "                self.creator[0].backward(self.grad.mm(c1.transpose()), self)\n",
    "                self.creator[1].backward(c0.transpose().mm(self.grad), self)\n",
    "            elif self.create_op[:4] == 'sum_':\n",
    "                dim = int(self.create_op[4:])\n",
    "                new = self.grad.expand(dim, self.creator[0].data.shape[dim])\n",
    "                self.creator[0].backward(new, self)\n",
    "            elif self.create_op[:7] == 'expand_':\n",
    "                dim = int(self.create_op[7:])\n",
    "                new = self.grad.sum(dim)\n",
    "                self.creator[0].backward(new, self)\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "       \n",
    "    def step(self, alpha):\n",
    "        if self.grad is None:\n",
    "            return\n",
    "        self.data -= self.grad.data*alpha\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        if isinstance(ind, int) or isinstance(ind, slice): # n是索引 or 切片\n",
    "            return self.data[ind]\n",
    "        elif isinstance(ind, tuple):\n",
    "            a_ind, b_ind = ind\n",
    "            return self.data[a_ind, b_ind]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# slice index operation for Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " [[0.84426575 0.85794562 0.84725174]\n",
      " [0.6235637  0.38438171 0.29753461]\n",
      " [0.05671298 0.27265629 0.47766512]]\n",
      "single index 0 for a:\n",
      " [0.84426575 0.85794562 0.84725174]\n",
      "single index 2 for a:\n",
      " [0.05671298 0.27265629 0.47766512]\n",
      "slice index 0:2 for a:\n",
      " [[0.84426575 0.85794562 0.84725174]\n",
      " [0.6235637  0.38438171 0.29753461]]\n",
      "tuple slice index 0:2, 0:2 for a:\n",
      " [[0.84426575 0.85794562]\n",
      " [0.6235637  0.38438171]]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.random.rand(3,3), autograd=False)\n",
    "print('a:\\n', a)\n",
    "print('single index 0 for a:\\n', a[0])\n",
    "print('single index 2 for a:\\n', a[2])\n",
    "print('slice index 0:2 for a:\\n', a[0:2])\n",
    "print('tuple slice index 0:2, 0:2 for a:\\n', a[0:2,0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## common op on Tensor, note the following example that b is same factor for d & e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a grad: [1 1 1 1 1]\n",
      "b grad: [2 2 2 2 2]\n",
      "c grad: [1 1 1 1 1]\n",
      "d grad: [1 1 1 1 1]\n",
      "e grad: [1 1 1 1 1]\n",
      "g grad: [2 2 2 2 2]\n",
      "h grad: [2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5], True)\n",
    "b = Tensor([2,2,2,2,2], True)\n",
    "c = Tensor([5,4,3,2,1], True)\n",
    "g = Tensor([6,2,3,4,5], True)\n",
    "h = Tensor([7,2,3,4,5], True)\n",
    "b = g + h\n",
    "d = a + b\n",
    "e = b + c\n",
    "f = d + e\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "print('a grad:', a.grad.data)\n",
    "print('b grad:', b.grad.data)\n",
    "print('c grad:', c.grad.data)\n",
    "print('d grad:', d.grad.data)\n",
    "print('e grad:', e.grad.data)\n",
    "print('g grad:', g.grad.data)  # Now, everything is OK\n",
    "print('h grad:', h.grad.data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# negative operation for Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True]\n",
      "b.grad: [-2 -2 -2 -2 -2]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5], autograd=True)\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\n",
    "c = Tensor([5,4,3,2,1], autograd=True)\n",
    "\n",
    "d = a + (-b)\n",
    "e = (-b) + c\n",
    "f = d + e\n",
    "\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "print(b.grad.data == np.array([-2,-2,-2,-2,-2]))\n",
    "print('b.grad:', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# op on expand & sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor(np.array([[1,2,3],\n",
    "                     [4,5,6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 9]\n",
      "[ True  True  True]\n",
      "[ 6 15]\n",
      "[ True  True]\n"
     ]
    }
   ],
   "source": [
    "b = x.sum(0)\n",
    "print(b)\n",
    "print(b.data == np.array([5, 7, 9]))\n",
    "\n",
    "b = x.sum(1)\n",
    "print(b)\n",
    "print(b.data == np.array([6, 15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 1 1 1]\n",
      "  [2 2 2 2]\n",
      "  [3 3 3 3]]\n",
      "\n",
      " [[4 4 4 4]\n",
      "  [5 5 5 5]\n",
      "  [6 6 6 6]]]\n",
      "[[[ True  True  True  True]\n",
      "  [ True  True  True  True]\n",
      "  [ True  True  True  True]]\n",
      "\n",
      " [[ True  True  True  True]\n",
      "  [ True  True  True  True]\n",
      "  [ True  True  True  True]]]\n"
     ]
    }
   ],
   "source": [
    "b = x.expand(dim=2, copies=4)\n",
    "print(b)\n",
    "print(b.data == np.array([[[1, 1, 1, 1],\n",
    "        [2, 2, 2, 2],\n",
    "        [3, 3, 3, 3]],\n",
    "\n",
    "       [[4, 4, 4, 4],\n",
    "        [5, 5, 5, 5],\n",
    "        [6, 6, 6, 6]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the grad for matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\n",
      " [[3 3]\n",
      " [7 7]]\n",
      "d:\n",
      " [[30 30]\n",
      " [70 70]]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([[1,2],[3,4]], autograd=True)\n",
    "b = Tensor([[2,1],[4,3]], autograd=True)\n",
    "c = a + b\n",
    "d = c.mm(c)\n",
    "print('c:\\n',c)\n",
    "print('d:\\n',d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c.grad:\n",
      " [[16 24]\n",
      " [16 24]]\n",
      "a.grad:\n",
      " [[16 24]\n",
      " [16 24]]\n",
      "b.grad:\n",
      " [[16 24]\n",
      " [16 24]]\n"
     ]
    }
   ],
   "source": [
    "d.backward(Tensor([[1,1],[1,1]]))\n",
    "print('c.grad:\\n', c.grad)\n",
    "print('a.grad:\\n', a.grad)\n",
    "print('b.grad:\\n', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train a nerual network, to simplify the network code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.066439994622396\n",
      "1.725208044893435\n",
      "0.970729785737745\n",
      "0.4484578158939851\n",
      "0.19705058205505\n",
      "0.11889682222130549\n",
      "0.07853709477623544\n",
      "0.050724621963417184\n",
      "0.03190534467093544\n",
      "0.019585091267885605\n"
     ]
    }
   ],
   "source": [
    "# the previous version\n",
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "alpha = 0.1\n",
    "data = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "target = np.array([[0],[1],[0],[1]])\n",
    "\n",
    "orgw_0_1 = np.random.rand(2,3)\n",
    "orgw_1_2 = np.random.rand(3,1)\n",
    "\n",
    "weights_0_1 = orgw_0_1.copy()\n",
    "weights_1_2 = orgw_1_2.copy()\n",
    "\n",
    "for i in range(10):\n",
    "    # Predict\n",
    "    layer_1 = data.dot(weights_0_1)\n",
    "    layer_2 = layer_1.dot(weights_1_2)\n",
    "    \n",
    "    # Compare\n",
    "    diff = (layer_2 - target)\n",
    "    sqdiff = (diff * diff)\n",
    "    loss = sqdiff.sum(0) # mean squared error loss\n",
    "\n",
    "    # Learn: this is the backpropagation piece\n",
    "    layer_1_grad = 2*diff.dot(weights_1_2.transpose())\n",
    "    weight_1_2_update = layer_1.transpose().dot(2*diff)\n",
    "    weight_0_1_update = data.transpose().dot(layer_1_grad)\n",
    "    \n",
    "    weights_1_2 -= weight_1_2_update * alpha\n",
    "    weights_0_1 -= weight_0_1_update * alpha\n",
    "    print(loss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43758721]\n",
      " [0.891773  ]\n",
      " [0.96366276]]\n"
     ]
    }
   ],
   "source": [
    "print(orgw_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28729166]\n",
      " [0.49330624]\n",
      " [0.77311751]]\n"
     ]
    }
   ],
   "source": [
    "print(weights_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00115038]\n",
      " [-0.01600303]\n",
      " [-0.10218782]]\n"
     ]
    }
   ],
   "source": [
    "print(weight_1_2_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [-0.0855257 ]\n",
      " [ 0.10838725]\n",
      " [ 0.02286155]]\n",
      "[[0.        ]\n",
      " [0.9144743 ]\n",
      " [0.10838725]\n",
      " [1.02286155]]\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(diff)\n",
    "print(layer_2)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_1_2:\n",
      " [[0.43758721]\n",
      " [0.891773  ]\n",
      " [0.96366276]]\n",
      "5.066439994622396\n",
      "1.725208044893435\n",
      "0.970729785737745\n",
      "0.4484578158939851\n",
      "0.19705058205505\n",
      "0.11889682222130549\n",
      "0.07853709477623544\n",
      "0.050724621963417184\n",
      "0.03190534467093544\n",
      "0.019585091267885605\n"
     ]
    }
   ],
   "source": [
    "# use the previous tensor framework, we can save the codes\n",
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "alpha = 0.1\n",
    "data = Tensor([[0,0],[0,1],[1,0],[1,1]], autograd=True)\n",
    "target = Tensor([[0],[1],[0],[1]], autograd=True)\n",
    "\n",
    "weights_0_1 = Tensor(orgw_0_1.copy(), autograd=True)\n",
    "weights_1_2 = Tensor(orgw_1_2.copy(), autograd=True)\n",
    "print('weights_1_2:\\n', weights_1_2)\n",
    "\n",
    "for i in range(10):\n",
    "    # Empyt grad\n",
    "    weights_0_1.zero_grad()\n",
    "    weights_1_2.zero_grad()\n",
    "\n",
    "    # Predict\n",
    "    layer_1 = data.mm(weights_0_1)\n",
    "    layer_2 = layer_1.mm(weights_1_2)\n",
    "    \n",
    "    # Compare\n",
    "    diff = layer_2 - target\n",
    "    sqdiff = diff * diff\n",
    "    loss = sqdiff.sum(0) # mean squared error loss\n",
    "\n",
    "    # Learn: this is the backpropagation piece\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    weights_1_2.data -= weights_1_2.grad.data * alpha\n",
    "    weights_0_1.data -= weights_0_1.grad.data * alpha\n",
    "    print(loss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28729166]\n",
      " [0.49330624]\n",
      " [0.77311751]]\n"
     ]
    }
   ],
   "source": [
    "print(weights_1_2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00115038]\n",
      " [-0.01600303]\n",
      " [-0.10218782]]\n",
      "[[-0.00115038]\n",
      " [-0.01600303]\n",
      " [-0.10218782]]\n"
     ]
    }
   ],
   "source": [
    "print(weights_1_2.grad)\n",
    "print(weight_1_2_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [-0.0855257 ]\n",
      " [ 0.10838725]\n",
      " [ 0.02286155]]\n",
      "[[0.        ]\n",
      " [0.9144743 ]\n",
      " [0.10838725]\n",
      " [1.02286155]]\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(diff)\n",
    "print(layer_2)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add optimizer class\n",
    "class SGD(object):\n",
    "    def __init__(self, parameters, alpha):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for one_param in self.parameters:\n",
    "            one_param.zero_grad()\n",
    "    \n",
    "    def step(self):\n",
    "        for one_param in self.parameters:\n",
    "            one_param.step(self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.066439994622396\n",
      "1.725208044893435\n",
      "0.970729785737745\n",
      "0.4484578158939851\n",
      "0.19705058205505\n",
      "0.11889682222130549\n",
      "0.07853709477623544\n",
      "0.050724621963417184\n",
      "0.03190534467093544\n",
      "0.019585091267885605\n"
     ]
    }
   ],
   "source": [
    "# another vesion for previous network\n",
    "# use the previous tensor framework, we can save the codes\n",
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor([[0,0],[0,1],[1,0],[1,1]], autograd=True)\n",
    "target = Tensor([[0],[1],[0],[1]], autograd=True)\n",
    "\n",
    "weights_0_1 = Tensor(orgw_0_1.copy(), autograd=True)\n",
    "weights_1_2 = Tensor(orgw_1_2.copy(), autograd=True)\n",
    "\n",
    "sgd = SGD(parameters=[weights_0_1, weights_1_2], alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    # Empyt grad\n",
    "    sgd.zero_grad()\n",
    "\n",
    "    # Predict\n",
    "    layer_2 = data.mm(weights_0_1).mm(weights_1_2)\n",
    "    \n",
    "    # Compare\n",
    "    diff = layer_2 - target\n",
    "    sqdiff = diff * diff\n",
    "    loss = sqdiff.sum(0) # mean squared error loss\n",
    "\n",
    "    # Learn: this is the backpropagation piece\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    sgd.step()\n",
    "    \n",
    "    print(loss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add layer support\n",
    "class ItemIdGen(object):\n",
    "    def __init__(self):\n",
    "        self.gen_id = {}\n",
    "        \n",
    "        self.default_items = ['Linear_Weights_', 'Linear_Bias_', 'Sequential_', 'RnnCell_', 'Embedding_Weights_']\n",
    "        for one_item in self.default_items:\n",
    "            self.gen_id[one_item] = 0\n",
    "    \n",
    "    def add_item(self, item):\n",
    "        self.default_items.append(item)\n",
    "        self.gen_id[item] = 0\n",
    "    \n",
    "    def get_next_id(self, item):\n",
    "        assert item in self.gen_id, 'The item %s is not in Id Gen base' % item\n",
    "        \n",
    "        cur_id = self.gen_id[item]\n",
    "        self.gen_id[item] += 1\n",
    "        \n",
    "        return cur_id\n",
    "\n",
    "item_id_gen = ItemIdGen()\n",
    "\n",
    "class Parameter(object):\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "        \n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "\n",
    "    def backward(self, grad):\n",
    "        self.value.backward(grad)\n",
    "    \n",
    "    def step(self, alpha):\n",
    "        self.value.step(alpha)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.value.zero_grad()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name+': '+str(self.value.__repr__())\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.parameters = []\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "\n",
    "    def __repr__(self):\n",
    "        layer_repr = ''\n",
    "        for one in self.get_parameters():\n",
    "            layer_repr += one.__repr__()+'\\n'\n",
    "        return layer_repr\n",
    "\n",
    "class LinearLayer(Layer):\n",
    "    def __init__(self, inns, outs, bias=True):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        \n",
    "        self.weights = Tensor(np.random.rand(inns, outs)*np.sqrt(2.0/inns), autograd=True)\n",
    "        self.parameters.append(Parameter(self.get_name('Linear_Weights_'), self.weights))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = Tensor(np.zeros(outs), autograd=True)\n",
    "            self.parameters.append(Parameter(self.get_name('Linear_Bias_'), self.bias))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def get_name(self, prefix):\n",
    "        suf_id = item_id_gen.get_next_id(prefix)\n",
    "        return prefix+str(suf_id)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x.mm(self.weights)\n",
    "        if self.bias is not None:\n",
    "            y += self.bias.expand(0, x.shape[0])\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add layers supportion(layer container)\n",
    "class  Sequential(Layer):\n",
    "    def __init__(self, layers=list()):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.layers = layers\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        layers_params = []\n",
    "        for one in self.layers:\n",
    "            layers_params += one.get_parameters()\n",
    "        return layers_params\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        p = x\n",
    "        for one in self.layers:\n",
    "            p = one.forward(p)\n",
    "            \n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.066439994622396\n",
      "1.725208044893435\n",
      "0.970729785737745\n",
      "0.4484578158939851\n",
      "0.19705058205505\n",
      "0.11889682222130549\n",
      "0.07853709477623544\n",
      "0.050724621963417184\n",
      "0.03190534467093544\n",
      "0.019585091267885605\n"
     ]
    }
   ],
   "source": [
    "# new version of previous neural network\n",
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor([[0,0],[0,1],[1,0],[1,1]], autograd=True)\n",
    "target = Tensor([[0],[1],[0],[1]], autograd=True)\n",
    "\n",
    "L1 = LinearLayer(2,3,bias=False)\n",
    "L2 = LinearLayer(3,1,bias=False)\n",
    "L1.weights.data = orgw_0_1.copy()     # comment this line to see different result\n",
    "L2.weights.data = orgw_1_2.copy()     # comment this line to see different result\n",
    "\n",
    "model = Sequential([L1, L2])\n",
    "sgd = SGD(parameters=model.get_parameters(), alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    # Empyt grad\n",
    "    sgd.zero_grad()\n",
    "\n",
    "    # Predict\n",
    "    layer_2 = model.forward(data)\n",
    "    \n",
    "    # Compare\n",
    "    diff = layer_2 - target\n",
    "    sqdiff = diff * diff\n",
    "    loss = sqdiff.sum(0) # mean squared error loss\n",
    "\n",
    "    # Learn: this is the backpropagation piece\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    sgd.step()\n",
    "    \n",
    "    print(loss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    pass\n",
    "\n",
    "class MSELoss(object):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "    \n",
    "    def __call__(self, pred, gt):\n",
    "        diff = pred - gt\n",
    "        sqdiff = diff * diff\n",
    "        loss = sqdiff.sum(0)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.066439994622396\n",
      "1.725208044893435\n",
      "0.970729785737745\n",
      "0.4484578158939851\n",
      "0.19705058205505\n",
      "0.11889682222130549\n",
      "0.07853709477623544\n",
      "0.050724621963417184\n",
      "0.03190534467093544\n",
      "0.019585091267885605\n"
     ]
    }
   ],
   "source": [
    "# new version of previous neural network\n",
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor([[0,0],[0,1],[1,0],[1,1]], autograd=True)\n",
    "target = Tensor([[0],[1],[0],[1]], autograd=True)\n",
    "\n",
    "L1 = LinearLayer(2,3,bias=False)\n",
    "L2 = LinearLayer(3,1,bias=False)\n",
    "L1.weights.data = orgw_0_1.copy()     # comment this line to see different result\n",
    "L2.weights.data = orgw_1_2.copy()     # comment this line to see different result\n",
    "\n",
    "model = Sequential([L1, L2])\n",
    "sgd = SGD(parameters=model.get_parameters(), alpha=0.1)\n",
    "criterion = MSELoss()\n",
    "\n",
    "for i in range(10):\n",
    "    # Empty grad\n",
    "    sgd.zero_grad()\n",
    "\n",
    "    # Predict\n",
    "    pred = model.forward(data)\n",
    "    \n",
    "    # Compare, mean squared error loss\n",
    "    loss = criterion(pred, target)\n",
    "\n",
    "    # Learn: this is the backpropagation piece\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    sgd.step()\n",
    "    \n",
    "    print(loss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we want to add 3 common operation in nerual network forward: non-linear(relu, sigmoid, tanh), index-select, cross-entropy support for class Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Tensor(object):\n",
    "    def __init__(self, data, autograd=False, creator=None, create_op=None, id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.shape = self.data.shape\n",
    "        self.creator = creator\n",
    "        self.create_op = create_op\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        self.children = {}\n",
    "        self.restore_children = {}\n",
    "        if id is None:\n",
    "            id = np.random.choice(10000)\n",
    "        self.id = id\n",
    "        \n",
    "        if creator is not None:\n",
    "            for c in creator:\n",
    "                if self.id not in c.children:\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "                # save the counter sync\n",
    "                if self.id not in c.restore_children:\n",
    "                    c.restore_children[self.id] = 1\n",
    "                else:\n",
    "                    c.restore_children[self.id] += 1\n",
    "\n",
    "                # for rnn\n",
    "                c.restore_graph() # just flow back it if needed,                    \n",
    "\n",
    "    def restore_graph(self):\n",
    "        if self.creator is not None and self.grad is not None: # it is a middle variable, and has ever been grad flowed\n",
    "            for c in self.creator:\n",
    "                assert self.id in c.children, 'we are in restore graph, so its id should be in creator children'\n",
    "                c.children[self.id] = c.restore_children[self.id]\n",
    "                c.restore_graph() # just flow back it if needed\n",
    "                \n",
    "            \n",
    "    def __add__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data+other.data, autograd=True, creator=(self, other), create_op='add')\n",
    "        return Tensor(self.data+other.data)\n",
    "        \n",
    "    def __neg__(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data*-1, autograd=True, creator=(self,), create_op='neg')\n",
    "        return Tensor(self.data*-1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data-other.data, autograd=True, creator=(self, other), create_op='sub')\n",
    "        return Tensor(self.data-other.data)\n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data*other.data, autograd=True, creator=(self, other), create_op='mul')\n",
    "        return Tensor(self.data*other.data)\n",
    "        \n",
    "    def sum(self, dim):\n",
    "        assert self.data.ndim>dim, 'axis %d is out of bounds for array of dimension %d' % (dim, self.data.ndim)\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.sum(dim), autograd=True, creator=(self,), create_op='sum_'+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def mean(self, dim):\n",
    "        assert self.data.ndim>dim, 'axis %d is out of bounds for array of dimension %d' % (dim, self.data.ndim)\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.mean(dim), autograd=True, creator=(self,), create_op='mean_'+str(dim))\n",
    "        return Tensor(self.data.mean(dim))\n",
    "    \n",
    "    def expand(self, dim, copies):\n",
    "        assert self.data.ndim>=dim, 'axis %d is out of bounds for array of dimension %d' % (dim, self.data.ndim)\n",
    "        if self.autograd:\n",
    "            return Tensor(np.expand_dims(self.data, axis=dim).repeat(copies, axis=dim), autograd=True, creator=(self,), create_op='expand_'+str(dim))\n",
    "        return Tensor(np.expand_dims(self.data, axis=dim).repeat(copies, axis=dim))\n",
    "    \n",
    "    def transpose(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.transpose(), autograd=True, creator=(self,), create_op='transpose')\n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, other):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.dot(other.data), autograd=True, creator=(self, other), create_op='mm')\n",
    "        return Tensor(self.data.dot(other.data))\n",
    "    \n",
    "    def relu(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(np.where(self.data>0, self.data, 0), autograd=True, creator=(self,), create_op='relu')\n",
    "        return Tensor(np.where(self.data>0, self.data, 0))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        new = 1/(1+np.exp(-self.data))\n",
    "        if self.autograd:\n",
    "            return Tensor(new, autograd=True, creator=(self,), create_op='sigmoid')\n",
    "        return Tensor(new)\n",
    "    \n",
    "    def tanh(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(np.tanh(self.data), autograd=True, creator=(self,), create_op='tanh')\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, inds):\n",
    "        if self.autograd:\n",
    "            new = Tensor(self.data[inds.data], autograd=True, creator=(self,), create_op='index_select')\n",
    "            new.ind_sel = inds\n",
    "            return new\n",
    "        return Tensor(self.data[inds.data])\n",
    "    \n",
    "    def cross_entropy(self, gt):\n",
    "        '''\n",
    "        gt is assumed to have the shape (n,1) or (n,), each element is in [0,n_classes-1] (n is the number of samples)\n",
    "        self.data is assumed to have shape (n, n_classes)\n",
    "        '''\n",
    "        assert self.data.ndim <= 2, 'the data''s dims should be smaller than 2'\n",
    "        assert gt.data.ndim <= 2, 'the target inds array''s ndim should be equal to 1'\n",
    "        \n",
    "        softmax_output = np.exp(self.data)/np.exp(self.data).sum(axis=self.data.ndim-1, keepdims=True)\n",
    "        gt_inds = gt.data.flatten()\n",
    "        softmax_output = softmax_output.reshape(len(gt_inds), -1)\n",
    "        loss = -np.log(softmax_output)[np.arange(len(gt_inds)), gt_inds]\n",
    "        loss = loss.mean()     \n",
    "\n",
    "        if self.autograd:\n",
    "            new = Tensor([loss], autograd=True, creator=(self,), create_op='cross_entropy')\n",
    "            new.gt = gt_inds\n",
    "            new.softmax_output = softmax_output\n",
    "            return new\n",
    "        return Tensor([loss])\n",
    "        \n",
    "    def check_creator_grad_count(self):\n",
    "        for c in self.children:\n",
    "            if self.children[c] != 0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def backward(self, grad=None, child_grad_node=None):\n",
    "        if not self.autograd:\n",
    "            return\n",
    "        \n",
    "        if child_grad_node is not None:\n",
    "            if self.children[child_grad_node.id] == 0:\n",
    "                assert self.children[child_grad_node.id] != 0, \\\n",
    "                'creator %d'' children %d has grad count == 0, backprop can has one pass' % (self.id, child_grad_node.id)\n",
    "            else:\n",
    "                self.children[child_grad_node.id] -= 1\n",
    "\n",
    "        if grad is None:\n",
    "            grad = Tensor(np.ones_like(self.data, dtype=np.float, shape=self.shape))\n",
    "            \n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "   \n",
    "        if self.creator is not None and self.check_creator_grad_count():\n",
    "            if self.create_op == 'add':\n",
    "                self.creator[0].backward(self.grad, self)\n",
    "                self.creator[1].backward(self.grad, self)\n",
    "            elif self.create_op == 'neg':\n",
    "                self.creator[0].backward(self.grad.__neg__(), self)\n",
    "            elif self.create_op == 'sub':\n",
    "                self.creator[0].backward(self.grad, self)\n",
    "                self.creator[1].backward(self.grad.__neg__(), self)\n",
    "            elif self.create_op == 'mul':\n",
    "                self.creator[0].backward(self.grad*self.creator[1], self)\n",
    "                self.creator[1].backward(self.grad*self.creator[0], self)\n",
    "            elif self.create_op == 'transpose':\n",
    "                self.creator[0].backward(self.grad.transpose(), self)\n",
    "            elif self.create_op == 'mm':\n",
    "                c0 = Tensor(self.creator[0].data, autograd=False)  # no auto grad\n",
    "                c1 = Tensor(self.creator[1].data, autograd=False)  # no auto grad\n",
    "                self.creator[0].backward(self.grad.mm(c1.transpose()), self)\n",
    "                self.creator[1].backward(c0.transpose().mm(self.grad), self)\n",
    "            elif self.create_op[:4] == 'sum_':\n",
    "                dim = int(self.create_op[4:])\n",
    "                new = self.grad.expand(dim, self.creator[0].data.shape[dim])\n",
    "                self.creator[0].backward(new, self)\n",
    "            elif self.create_op[:7] == 'expand_':\n",
    "                dim = int(self.create_op[7:])\n",
    "                new = self.grad.sum(dim)\n",
    "                self.creator[0].backward(new, self)\n",
    "            elif self.create_op == 'relu':\n",
    "                factor = np.where(self.data>0, 1, 0)\n",
    "                self.creator[0].backward(self.grad*factor, self)\n",
    "            elif self.create_op == 'sigmoid':\n",
    "                new = Tensor(self.data*(1-self.data))*self.grad\n",
    "                self.creator[0].backward(new, self)\n",
    "            elif self.create_op == 'tanh':\n",
    "                new = Tensor(1-self.data*self.data)*self.grad\n",
    "                self.creator[0].backward(new, self)\n",
    "            elif self.create_op == 'index_select':\n",
    "                new_grad = np.zeros_like(self.creator[0].data)\n",
    "                inds = self.ind_sel.data.flatten()\n",
    "                grad_ = self.grad.data.reshape(len(inds),-1)\n",
    "                for i in range(len(inds)):\n",
    "                    new_grad[inds[i]] += grad_[i]\n",
    "                self.creator[0].backward(Tensor(new_grad), self)\n",
    "            elif self.create_op == 'cross_entropy':\n",
    "                new_grad = self.softmax_output.copy()\n",
    "                new_grad[np.arange(len(self.gt)), self.gt] -= 1\n",
    "                new_grad = Tensor(new_grad)*self.grad\n",
    "                self.creator[0].backward(new_grad, self)\n",
    "            elif self.create_op[:5] == 'mean_':\n",
    "                dim = int(self.create_op[5:])\n",
    "                copies = self.creator[0].data.shape[dim]\n",
    "                new = self.grad.expand(dim, copies)\n",
    "                new.data = new.data*1.0/copies\n",
    "                self.creator[0].backward(new, self)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "       \n",
    "    def step(self, alpha):\n",
    "        if self.grad is None:\n",
    "            return\n",
    "        self.data -= self.grad.data*alpha\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        if isinstance(ind, int) or isinstance(ind, slice): # n是索引 or 切片\n",
    "            return self.data[ind]\n",
    "        elif isinstance(ind, tuple):\n",
    "            a_ind, b_ind = ind\n",
    "            return self.data[a_ind, b_ind]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sigmoid & tanh & word embedding layer\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x.sigmoid()\n",
    "    \n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x.tanh()\n",
    "\n",
    "class EmbeddingLayer(Layer):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        w = np.random.rand(vocab_size, hidden_size)\n",
    "        self.embedding_weights = Tensor(w, autograd=True)\n",
    "        self.parameters.append(self.embedding_weights)\n",
    "        \n",
    "    def forward(self, words):\n",
    "        return self.embedding_weights.index_select(words)\n",
    "    \n",
    "class CrossEntropyLoss(Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def __call__(self, pred, target):\n",
    "        '''\n",
    "        The function combines softmax & cross entropy\n",
    "        '''\n",
    "        return pred.cross_entropy(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op on index_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(np.eye(5), autograd=True)\n",
    "x.index_select(Tensor([[1,2,3],[2,3,4]])).backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[0.88533766 0.67987946 0.45612977 0.48340862]\n",
      " [0.78873943 0.22944183 0.8802976  0.31369239]]\n",
      "y: [1.26928082]\n",
      "x softmax output: [[0.31904777 0.25979234 0.20770794 0.21345196]\n",
      " [0.30401173 0.17377627 0.33316054 0.18905146]]\n",
      "gt dist matrix: [1 0]\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(np.random.rand(2,4), autograd=True)\n",
    "gt = Tensor(np.array([1,0]))\n",
    "y = x.cross_entropy(gt)\n",
    "print('x:',x)\n",
    "print('y:', y)\n",
    "print('x'' softmax output:', y.softmax_output)\n",
    "print('gt dist matrix:', y.gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.31904777 -0.74020766  0.20770794  0.21345196]\n",
      " [-0.69598827  0.17377627  0.33316054  0.18905146]]\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.237625619852779\n",
      "0.9392089326123719\n",
      "0.7078099783922267\n",
      "0.5197284758168169\n",
      "0.36610028172821235\n",
      "0.2551263989440395\n",
      "0.18256411199440728\n",
      "0.13620932854196413\n",
      "0.10586065235996045\n",
      "0.08517312638904313\n",
      "0.0704825128526079\n",
      "0.05966252515717865\n",
      "0.05144032172950523\n",
      "0.04502472491541149\n",
      "0.03990542615281734\n",
      "0.03574191459555345\n",
      "0.032299977764343464\n",
      "0.02941412761239609\n",
      "0.026964621528852706\n",
      "0.02486297336106265\n"
     ]
    }
   ],
   "source": [
    "# put all together(without cross entropy)\n",
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "embed = EmbeddingLayer(5,3)\n",
    "model = Sequential([embed, Tanh(), LinearLayer(3,1), Sigmoid()])\n",
    "criterion = MSELoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.5)\n",
    "\n",
    "for i in range(20):\n",
    "    # zero grad\n",
    "    optim.zero_grad()\n",
    "    # predict\n",
    "    pred = model.forward(data)\n",
    "    # loss\n",
    "    loss = criterion(pred, target)\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    # step grad\n",
    "    optim.step()\n",
    "    # print loss\n",
    "    print(loss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network with cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7015193954153844\n",
      "0.661821171510658\n",
      "0.6180748425684603\n",
      "0.5504361740525101\n",
      "0.4526345603907933\n",
      "0.3424455258888502\n",
      "0.2500484390577531\n",
      "0.18608274753695986\n",
      "0.14402676254263264\n",
      "0.11577686966363329\n"
     ]
    }
   ],
   "source": [
    "# put all together(with cross entropy)\n",
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "embed = EmbeddingLayer(5,3)\n",
    "model = Sequential([embed, Tanh(), LinearLayer(3,2)])\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.25)\n",
    "\n",
    "for i in range(10):\n",
    "    # zero grad\n",
    "    optim.zero_grad()\n",
    "    # predict\n",
    "    pred = model.forward(data)\n",
    "    # loss\n",
    "    loss = criterion(pred, target)\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    # step grad\n",
    "    optim.step()\n",
    "    # print loss\n",
    "    print(loss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.08439576e+00 -1.64601813e-03]\n",
      " [-1.19118162e+00  9.18184926e-01]\n",
      " [ 2.08439576e+00 -1.64601813e-03]\n",
      " [-1.19118162e+00  9.18184926e-01]]\n",
      "[[0.88953909 0.11046091]\n",
      " [0.10818977 0.89181023]\n",
      " [0.88953909 0.11046091]\n",
      " [0.10818977 0.89181023]]\n",
      "target: [[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "predict: [0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "def softmax(pred):\n",
    "    return np.exp(pred.data)/np.exp(pred.data).sum(axis=1, keepdims=True)\n",
    "def predict(softmax_output):\n",
    "    return softmax_output.argmax(axis=1)\n",
    "print(pred)\n",
    "print(softmax(pred))\n",
    "print('target:',target)\n",
    "print('predict:', predict(softmax(pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now it is for rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(Layer):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size, activation='sigmoid'):\n",
    "        super(RNNCell, self).__init__()\n",
    "        \n",
    "        self.input_weights = LinearLayer(embedding_size, hidden_size)\n",
    "        self.hidden_state = LinearLayer(hidden_size, hidden_size)\n",
    "        self.output_weights = LinearLayer(hidden_size, vocab_size)\n",
    "        \n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = Sigmoid()\n",
    "        else:\n",
    "            self.activation = Tanh()\n",
    "            \n",
    "        self.parameters = self.input_weights.get_parameters()+self.hidden_state.get_parameters()+self.output_weights.get_parameters()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        word_input = self.input_weights.forward(input)\n",
    "        cur_hidden = self.hidden_state.forward(hidden) + word_input\n",
    "        cur_hidden = self.activation.forward(cur_hidden)\n",
    "        output = self.output_weights.forward(cur_hidden)\n",
    "        \n",
    "        return output, cur_hidden\n",
    "\n",
    "class RNN_Model(Sequential):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size):\n",
    "        super(RNN_Model, self).__init__()\n",
    "        \n",
    "        self.word_embedding = EmbeddingLayer(vocab_size, embedding_size)\n",
    "        self.rnn = RNNCell(embedding_size, hidden_size, vocab_size)\n",
    "        \n",
    "        self.add(self.word_embedding)\n",
    "        self.add(self.rnn)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        word_embeds = self.word_embedding.forward(input)\n",
    "        output, hidden = self.rnn.forward(word_embeds, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Dataset:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.start_token = '<START>'\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.word2ind = {}\n",
    "        self.ind2word = {}\n",
    "        self.corpus = []\n",
    "    \n",
    "    def remove_noneed(self, r):\n",
    "        no_needs = ['0','1','2','3','4','5','6','7','8','9','\\n','\\t1', '\\t2','\\t3','\\t4','\\t5''\\t6','\\t7','\\t8','\\t9', '.','?','\\t']\n",
    "        for n in no_needs:\n",
    "            r = r.replace(n, '')\n",
    "        return r\n",
    "\n",
    "    def parse(self):\n",
    "        f = open(path)\n",
    "        raw = f.readlines()\n",
    "        f.close()\n",
    "        self.corpus = [self.remove_noneed(r).split(' ')[1:] for r in raw]\n",
    "        \n",
    "        # add pad & start token\n",
    "        ind = 0\n",
    "        self.word2ind[self.pad_token] = ind\n",
    "        self.ind2word[ind] = self.pad_token\n",
    "        ind = 1\n",
    "        self.word2ind[self.start_token] = ind\n",
    "        self.ind2word[ind] = self.start_token\n",
    "        ind = 2\n",
    "        \n",
    "        self.max_len = 0\n",
    "        for sent in self.corpus:\n",
    "            if self.max_len < len(sent):\n",
    "                self.max_len = len(sent)\n",
    "                \n",
    "            for w in sent:\n",
    "                if w not in self.word2ind:\n",
    "                    self.word2ind[w] = ind\n",
    "                    self.ind2word[ind] = w\n",
    "                    ind += 1\n",
    "        #self.max_len += 1  # add start token\n",
    "        np.random.shuffle(self.corpus)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent = self.corpus[idx]\n",
    "        sent_inds = np.array([self.word2ind[self.pad_token]]*(self.max_len-len(sent))+[self.word2ind[w] for w in sent])\n",
    "        return sent_inds\n",
    "    \n",
    "    def get_sent(self, idx):\n",
    "        return self.corpus[idx]\n",
    "    \n",
    "    def get_vob_len(self):\n",
    "        return len(self.word2ind)\n",
    "    \n",
    "    def get_start_token(self):\n",
    "        return self.start_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do explore in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length is 6\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "path = '../tasksv11/en/qa1_single-supporting-fact_train.txt'\n",
    "qa_ds = QA_Dataset(path)\n",
    "qa_ds.parse()\n",
    "print('max length is', qa_ds.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'travelled', 'to', 'the', 'kitchen'] ind: [ 0  2 18  4  5 20]\n",
      "we have 3000 training samples.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "train_dataloader = DataLoader(qa_ds, batch_size=batch_size, shuffle=True)  #20000\n",
    "print(qa_ds.get_sent(0), 'ind:', qa_ds[0])\n",
    "print('we have', len(qa_ds),'training samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 14, 18,  4,  5, 19],\n",
      "        [ 0,  0, 10, 11, 14,  6],\n",
      "        [ 0, 14, 17,  4,  5,  6],\n",
      "        [ 0,  2,  8,  4,  5, 15],\n",
      "        [ 0,  0, 10, 11,  7,  6],\n",
      "        [ 0, 14,  8,  4,  5, 20],\n",
      "        [ 0,  0, 10, 11, 12,  9],\n",
      "        [ 0, 14, 18,  4,  5, 15],\n",
      "        [ 0, 14,  3,  4,  5, 15],\n",
      "        [ 0, 14,  8,  4,  5,  9],\n",
      "        [ 0,  7, 17,  4,  5, 16],\n",
      "        [ 0,  0, 10, 11, 12,  9],\n",
      "        [ 0,  2,  8,  4,  5,  6],\n",
      "        [ 0,  2,  8,  4,  5, 20],\n",
      "        [ 0,  0, 10, 11, 14,  9],\n",
      "        [ 0,  0, 10, 11, 12,  9]])\n",
      "----------------------------------------------------------------\n",
      "0 <PAD> Sandra travelled to the bedroom \n",
      "1 <PAD> <PAD> Where is Sandra bathroom \n",
      "2 <PAD> Sandra journeyed to the bathroom \n",
      "3 <PAD> Mary went to the garden \n",
      "4 <PAD> <PAD> Where is John bathroom \n",
      "5 <PAD> Sandra went to the kitchen \n",
      "6 <PAD> <PAD> Where is Daniel hallway \n",
      "7 <PAD> Sandra travelled to the garden \n",
      "8 <PAD> Sandra moved to the garden \n",
      "9 <PAD> Sandra went to the hallway \n",
      "10 <PAD> John journeyed to the office \n",
      "11 <PAD> <PAD> Where is Daniel hallway \n",
      "12 <PAD> Mary went to the bathroom \n",
      "13 <PAD> Mary went to the kitchen \n",
      "14 <PAD> <PAD> Where is Sandra hallway \n",
      "15 <PAD> <PAD> Where is Daniel hallway \n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_dataloader = DataLoader(qa_ds, batch_size=batch_size, shuffle=True)  #20000\n",
    "batch_x = iter(train_dataloader).next()\n",
    "print(batch_x)\n",
    "print('-'*64)\n",
    "for idx, one_sent in enumerate(batch_x):\n",
    "    print(idx, sep ='.', end=' ')\n",
    "    for one_word in one_sent:\n",
    "        print(qa_ds.ind2word[one_word.item()],sep=' ', end=' ')\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-e9856e1bd2b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vob_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-2aeef59334b9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embedding_size, hidden_size, vocab_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRNN_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN_Model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-4194398bde4c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mclass\u001b[0m  \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "epoches = 15\n",
    "batch_size = 100\n",
    "path = '../tasksv11/en/qa1_single-supporting-fact_train.txt'\n",
    "qa_ds = QA_Dataset(path)\n",
    "qa_ds.parse()\n",
    "train_dataloader = DataLoader(qa_ds, batch_size=batch_size, shuffle=True)  #20000\n",
    "\n",
    "word_embedding_size = 64\n",
    "hidden_size = 64\n",
    "vocab_size = qa_ds.get_vob_len()\n",
    "model = RNN_Model(word_embedding_size, hidden_size, vocab_size)\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.001)\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for one_bs in train_dataloader:\n",
    "        hidden = Tensor(np.zeros((batch_size, hidden_size)), autograd=True)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        for i in range(qa_ds.max_len-1):\n",
    "            x = one_bs[:,i]\n",
    "            y = one_bs[:,i+1]\n",
    "\n",
    "            input = Tensor(x.detach().cpu().numpy(), autograd=True)\n",
    "            target = Tensor(y.detach().cpu().numpy(), autograd=True)\n",
    "\n",
    "            pred, hidden = model.forward(input, hidden)\n",
    "            \n",
    "            loss = criterion(pred, target)\n",
    "            acc = (pred.data.argmax(axis=1) == target.data).mean()\n",
    "            total_loss += loss[0]\n",
    "            total_acc += acc\n",
    "            counter += 1\n",
    "        \n",
    "            loss.backward()\n",
    "        optim.step()\n",
    "            \n",
    "    print('In epoch %d, nn gets loss %.4f, acc %.4f' % (epoch, total_loss/counter, total_acc/counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.data.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  6,  6, 19,  9,  9,  9,  6,  6, 15,  6,  9, 16,  6,  6, 15,  9,\n",
       "        6, 15, 16,  6, 16, 20,  6, 15, 20,  9, 20,  9,  6,  6, 16, 15, 15,\n",
       "       16, 16, 20, 16, 16, 19, 20, 19, 15,  6, 20, 20, 15, 16, 19, 20,  6,\n",
       "       15, 20,  9, 16,  6, 19, 20, 15, 15, 15, 20,  6,  6, 15, 15,  9, 15,\n",
       "        6,  9, 19, 15, 16,  6, 16, 19, 15,  9, 15, 20,  9, 15, 15,  9, 20,\n",
       "        6, 20, 16,  6, 15,  6,  6, 20, 16, 19, 20, 20,  6, 19, 19])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get one word, then predict one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gt sentence: Mary moved to the garden\n",
      "Get <PAD> Pred: <PAD> Gt: Mary\n",
      "Get Mary Pred: journeyed Gt: moved\n",
      "Get moved Pred: to Gt: to\n",
      "Get to Pred: the Gt: the\n",
      "Get the Pred: bathroom Gt: garden\n",
      "Pred sentence: <PAD> journeyed to the bathroom \n",
      "--------------------------------\n",
      "Gt sentence: Where is Mary kitchen\n",
      "Get <PAD> Pred: <PAD> Gt: <PAD>\n",
      "Get <PAD> Pred: Where Gt: Where\n",
      "Get Where Pred: is Gt: is\n",
      "Get is Pred: Mary Gt: Mary\n",
      "Get Mary Pred: bathroom Gt: kitchen\n",
      "Pred sentence: <PAD> Where is Mary bathroom \n",
      "--------------------------------\n",
      "Gt sentence: John went to the kitchen\n",
      "Get <PAD> Pred: <PAD> Gt: John\n",
      "Get John Pred: journeyed Gt: went\n",
      "Get went Pred: to Gt: to\n",
      "Get to Pred: the Gt: the\n",
      "Get the Pred: bathroom Gt: kitchen\n",
      "Pred sentence: <PAD> journeyed to the bathroom \n",
      "--------------------------------\n",
      "Gt sentence: Where is Daniel office\n",
      "Get <PAD> Pred: <PAD> Gt: <PAD>\n",
      "Get <PAD> Pred: Where Gt: Where\n",
      "Get Where Pred: is Gt: is\n",
      "Get is Pred: Mary Gt: Daniel\n",
      "Get Daniel Pred: bathroom Gt: office\n",
      "Pred sentence: <PAD> Where is Mary bathroom \n",
      "--------------------------------\n",
      "Gt sentence: Where is Sandra bedroom\n",
      "Get <PAD> Pred: <PAD> Gt: <PAD>\n",
      "Get <PAD> Pred: Where Gt: Where\n",
      "Get Where Pred: is Gt: is\n",
      "Get is Pred: Mary Gt: Sandra\n",
      "Get Sandra Pred: bathroom Gt: bedroom\n",
      "Pred sentence: <PAD> Where is Mary bathroom \n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_num = 5\n",
    "for i in range(test_num):\n",
    "    sent = qa_ds[i]\n",
    "    print('Gt sentence:', ' '.join(qa_ds.get_sent(i)))\n",
    "    pred_sent = \"\"\n",
    "    hidden = Tensor(np.zeros((1, hidden_size)), autograd=True)\n",
    "    for j in range(len(sent)-1):\n",
    "        x = sent[j]\n",
    "        gt = sent[j+1]\n",
    "        \n",
    "        input = Tensor([x], autograd=True)\n",
    "        pred, hidden = model.forward(input, hidden)\n",
    "        pred_y = pred.data.argmax(axis=1)[0]\n",
    "        print('Get', qa_ds.ind2word[x],'Pred:', qa_ds.ind2word[pred_y], 'Gt:', qa_ds.ind2word[gt])\n",
    "        pred_sent += qa_ds.ind2word[pred_y] +\" \"\n",
    "    print('Pred sentence:', pred_sent)\n",
    "    print('-'*32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get words(or context, or sequence), then predict one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gt sentence: Mary moved to the garden\n",
      "Context: <PAD> Mary moved to the  Pred: bathroom -- Gt: garden\n",
      "----------------------------------------------------------------\n",
      "Gt sentence: Where is Mary kitchen\n",
      "Context: <PAD> <PAD> Where is Mary  Pred: bathroom -- Gt: kitchen\n",
      "----------------------------------------------------------------\n",
      "Gt sentence: John went to the kitchen\n",
      "Context: <PAD> John went to the  Pred: bathroom -- Gt: kitchen\n",
      "----------------------------------------------------------------\n",
      "Gt sentence: Where is Daniel office\n",
      "Context: <PAD> <PAD> Where is Daniel  Pred: bathroom -- Gt: office\n",
      "----------------------------------------------------------------\n",
      "Gt sentence: Where is Sandra bedroom\n",
      "Context: <PAD> <PAD> Where is Sandra  Pred: bathroom -- Gt: bedroom\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_num = 5\n",
    "for i in range(test_num):\n",
    "    sent = qa_ds[i]\n",
    "    print('Gt sentence:', ' '.join(qa_ds.get_sent(i)))\n",
    "    context = \"\"\n",
    "    hidden = Tensor(np.zeros((1, hidden_size)), autograd=True)\n",
    "    for j in range(len(sent)-1):\n",
    "        x = sent[j]\n",
    "        gt = sent[j+1]\n",
    "        context += qa_ds.ind2word[x]+\" \"\n",
    "        \n",
    "        input = Tensor([x], autograd=True)\n",
    "        pred, hidden = model.forward(input, hidden)\n",
    "        \n",
    "    pred_y = pred.data.argmax(axis=1)[0]\n",
    "    print('Context:', context,'Pred:', qa_ds.ind2word[pred_y], '-- Gt:', qa_ds.ind2word[gt])\n",
    "    print('-'*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}