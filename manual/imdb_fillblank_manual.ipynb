{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "reviews_path = '../datasets/IMDB/reviews.txt'\n",
    "labels_path = '../datasets/IMDB/labels.txt'\n",
    "\n",
    "class imdb_reviews:\n",
    "    def __init__(self, reviews_path, labels_path, train):\n",
    "        self.reviews_path = reviews_path\n",
    "        self.labels_path = labels_path\n",
    "        self.is_train = train\n",
    "        self.test_num = 1000\n",
    "        \n",
    "        self.reviews = []\n",
    "        self.labels = []\n",
    "        self.word2ind = []\n",
    "        self.vocab = {}\n",
    "        self.vocab_reverse = {}\n",
    "        \n",
    "    def parse_reviews(self):\n",
    "        f = open(self.reviews_path)\n",
    "        raw_reviews = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "        f = open(self.labels_path)\n",
    "        raw_labels = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "        raw_reviews = [r.split() for r in raw_reviews]\n",
    "        ind = 0\n",
    "        for r in raw_reviews:\n",
    "            for w in r:\n",
    "                if w not in self.vocab:\n",
    "                    self.vocab[w] = ind\n",
    "                    self.vocab_reverse[ind] = w\n",
    "                    ind += 1\n",
    "                    \n",
    "        if self.is_train:\n",
    "            raw_reviews = raw_reviews[:-1*self.test_num]\n",
    "        else:\n",
    "            raw_reviews = raw_reviews[-1*self.test_num:]\n",
    "        \n",
    "        if self.is_train:\n",
    "            raw_labels = raw_labels[:-1*self.test_num]\n",
    "        else:\n",
    "            raw_labels = raw_labels[-1*self.test_num:]\n",
    "        raw_labels = [r[:-1] for r in raw_labels]\n",
    "        \n",
    "        self.word2ind = list(self.vocab_reverse.keys())\n",
    "        self.reviews = raw_reviews\n",
    "        self.labels = raw_labels\n",
    "    \n",
    "    def get_vocab_len(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def get_word_ind(self, word):\n",
    "        return self.vocab[word]\n",
    "    \n",
    "    def get_ind_word(self, ind):\n",
    "        return self.vocab_reverse[ind]\n",
    "    \n",
    "    def gen_random_word(self, win=2):\n",
    "        targets = random.choices(self.word2ind, k=win)\n",
    "        return targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        r = np.array([self.vocab[w] for w in self.reviews[idx]])\n",
    "        l = 1 if self.labels[idx]=='positive' else 0\n",
    "        \n",
    "        return r, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 24000 train samples, and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "train_ds = imdb_reviews(reviews_path, labels_path, True)\n",
    "train_ds.parse_reviews()\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)  #24000\n",
    "\n",
    "test_ds = imdb_reviews(reviews_path, labels_path, False)\n",
    "test_ds.parse_reviews()\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)    #1000\n",
    "\n",
    "print('we have %d train samples, and %d test samples' % (len(train_dataloader), len(test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocab is 74073\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "win = 2\n",
    "negative = 5\n",
    "target_y = np.zeros((1,negative+1))\n",
    "target_y[0,0] = 1\n",
    "\n",
    "vocab_len = train_ds.get_vocab_len()\n",
    "hidden_size = 128\n",
    "\n",
    "layer0_w = np.random.normal(0, 0.02, (vocab_len, hidden_size))-0.01\n",
    "layer1_w = np.random.normal(0, 0.02, (vocab_len, hidden_size))-0.01\n",
    "print('length of vocab is', vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-1*x))\n",
    "\n",
    "def calc_acc(pred, gt):\n",
    "    y = (pred>=0.5).astype(np.int)\n",
    "    return np.mean(y==gt)\n",
    "\n",
    "def cross_entropy_loss(pred, gt):\n",
    "    loss = -gt*np.log(pred)-(1-gt)*np.log(1-pred)\n",
    "    return np.mean(loss)\n",
    "\n",
    "# previous one may have problem, when pred = 0 or 1\n",
    "def cross_entropy_loss2(pred, gt):\n",
    "    loss = 0.0\n",
    "    for one_pred, one_gt in zip(pred[0], gt[0]):\n",
    "        cur_loss = 0\n",
    "        if one_gt == 1:\n",
    "            if one_pred == 0:\n",
    "                one_pred += 1e+7\n",
    "            cur_loss = -1*np.log(one_pred)\n",
    "        else:\n",
    "            if 1-one_pred == 0:\n",
    "                one_pred -= 1e-7\n",
    "            cur_loss = -1*np.log(1-one_pred)\n",
    "        loss += cur_loss\n",
    "\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(left, right, target_sample):\n",
    "    word_embed = np.mean(layer0_w[left+right], axis=0, keepdims=True)\n",
    "\n",
    "    pred = word_embed.dot(layer1_w[target_sample].T)\n",
    "    pred = sigmoid(pred)\n",
    "    \n",
    "    return pred, word_embed\n",
    "\n",
    "def backward(x, word_embed, pred, gt):\n",
    "    num = pred.shape[0]\n",
    "    delta = (pred-gt)/num\n",
    "    layer1_delta = word_embed.T.dot(delta)  # hidden_size*(w+1)\n",
    "    \n",
    "    w_delta = delta.dot(layer1_w[x])        # 1*hidden_size\n",
    "    \n",
    "    return layer1_delta.T, w_delta\n",
    "\n",
    "def step(layer1_delta, w_delta, left, right, target_sample):\n",
    "    global layer0_w, layer1_w\n",
    "    \n",
    "    layer1_w[target_sample] -= alpha*layer1_delta\n",
    "    layer0_w[left+right] -= alpha*w_delta\n",
    "        \n",
    "def test():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for x,y in test_dataloader:\n",
    "        x = x.cpu().numpy()\n",
    "        y = y.cpu().numpy()\n",
    "        \n",
    "        pred, _ = forward(x)\n",
    "        loss += cross_entropy_loss2(pred, y)\n",
    "        accuracy += calc_acc(pred, y)\n",
    "    \n",
    "    return loss/len(test_dataloader), accuracy/len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Progress:0.999979166666666645"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "train_ds_num = len(train_dataloader)\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    total_counter = 0\n",
    "    for idx, (x,y) in enumerate(train_dataloader):\n",
    "        x = x.cpu().numpy().tolist()[0]\n",
    "        y = y.cpu().numpy()\n",
    "        sentence_len = len(x)\n",
    "        \n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        counter = 0\n",
    "        for i, target_word in enumerate(x):\n",
    "            left = x[max(0, i-win):i]\n",
    "            right = x[i+1:min(sentence_len, i+1+win)]\n",
    "            target_sample = [target_word]+train_ds.gen_random_word(negative) # negative sampling\n",
    "\n",
    "            #counter += 1\n",
    "            pred, word_embed = forward(left, right, target_sample)\n",
    "            #loss += cross_entropy_loss2(pred, target_y)\n",
    "            #accuracy += calc_acc(pred, target_y)\n",
    "        \n",
    "            layer1_delta, w_delta = backward(target_sample, word_embed, pred, target_y)\n",
    "            step(layer1_delta, w_delta, left, right, target_sample)\n",
    "        #print('in cur sentence, train loss: %.4f, train acc: %.4f' % (loss/counter, accuracy/counter))\n",
    "        \n",
    "        #total_counter += counter\n",
    "        #total_loss += loss\n",
    "        #total_accuracy += accuracy\n",
    "        \n",
    "        if (idx+1) % 200 == 0:\n",
    "            sys.stdout.write('\\rCurrent Progress:'+str((idx+epoch*train_ds_num)/(epochs*train_ds_num)))\n",
    "\n",
    "    #print('in epoch %d, train loss: %.4f, train acc: %.4f' % (epoch, total_loss/total_counter, total_accuracy/total_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99976412, 0.02828265, 0.00983641, 0.01533648, 0.14435772,\n",
       "        0.02797571]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 48064, 37975, 29847, 31874, 10907]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_similar(w_vec1, words_embed, topk=10):\n",
    "    similar_ratio = {}\n",
    "    for ind, one_vec in enumerate(words_embed):\n",
    "        s = np.sum((w_vec1-one_vec)**2)**0.5\n",
    "        similar_ratio[ind] = s\n",
    "    similar = sorted(similar_ratio.items(), key=lambda x:x[1])    \n",
    "    return similar[:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful ' similar top 10 is:\n",
      "beautiful : 0.0\n",
      "creepy : 5.483081724418267\n",
      "gorgeous : 5.526548973779048\n",
      "simple : 5.693494642706597\n",
      "cute : 5.783632045118047\n",
      "hilarious : 5.872267160125815\n",
      "amazing : 5.938913670649891\n",
      "laughable : 5.95729117686135\n",
      "bizarre : 6.009242231104356\n",
      "design : 6.042151794856582\n"
     ]
    }
   ],
   "source": [
    "word = 'beautiful'\n",
    "word_ind = train_ds.get_word_ind(word)\n",
    "word_vec = layer0_w[word_ind,:]\n",
    "similar_topk = calc_similar(word_vec, layer0_w)\n",
    "print(word,'\\' similar top 10 is:')\n",
    "for one_similar in similar_topk:\n",
    "    print(train_ds.get_ind_word(one_similar[0]),':',one_similar[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible ' similar top 10 is:\n",
      "terrible : 0.0\n",
      "horrible : 3.6985681519161497\n",
      "ridiculous : 4.113903726515322\n",
      "fantastic : 4.274580778626832\n",
      "laughable : 4.309029905230709\n",
      "unbelievable : 4.70128648631396\n",
      "amazing : 4.753286227954823\n",
      "weak : 4.78175429792215\n",
      "ok : 4.806621719440567\n",
      "amateurish : 4.823310725853438\n"
     ]
    }
   ],
   "source": [
    "word = 'terrible'\n",
    "word_ind = train_ds.get_word_ind(word)\n",
    "word_vec = layer0_w[word_ind,:]\n",
    "similar_topk = calc_similar(word_vec, layer0_w)\n",
    "print(word,'\\' similar top 10 is:')\n",
    "for one_similar in similar_topk:\n",
    "    print(train_ds.get_ind_word(one_similar[0]),':',one_similar[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is King-Man+Woman ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(words_embed, positive=['terrible','good'], negative=['bad'], topk=10):\n",
    "    query_word_embed = np.zeros((1, hidden_size))\n",
    "    for one_word in positive:\n",
    "        word_ind = train_ds.get_word_ind(one_word)\n",
    "        query_word_embed += words_embed[word_ind]\n",
    "    \n",
    "    for one_word in negative:\n",
    "        word_ind = train_ds.get_word_ind(one_word)\n",
    "        query_word_embed -= words_embed[word_ind]\n",
    "    \n",
    "    similar_ratio = {}\n",
    "    for ind, one_vec in enumerate(words_embed):\n",
    "        s = np.sum((query_word_embed-one_vec)**2)**0.5\n",
    "        similar_ratio[ind] = s\n",
    "    similar = sorted(similar_ratio.items(), key=lambda x:x[1])    \n",
    "    return similar[:topk]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# terrible+good-bad = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good             :   9.078256428583233\n",
      "great            :   10.527742974507317\n",
      "fine             :   10.558165287023504\n",
      "wonderful        :   10.842303564946338\n",
      "interesting      :   10.993378847357336\n",
      "excellent        :   11.06400691229927\n",
      "amazing          :   11.171037603024756\n",
      "fantastic        :   11.33046015876567\n",
      "strong           :   11.366604138898591\n",
      "nice             :   11.380902566844185\n"
     ]
    }
   ],
   "source": [
    "similar = analogy(layer0_w, ['terrible','good'], ['bad'])\n",
    "for one in similar:\n",
    "    word = train_ds.get_ind_word(one[0]) \n",
    "    print(word,' '*(15-len(word)),':  ', one[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 伊丽莎白＋他－她＝？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simon            :   8.932276197819354\n",
      "dylan            :   8.937588886962676\n",
      "hawke            :   8.949760044293816\n",
      "perry            :   9.040179995689215\n",
      "wallace          :   9.04926009813364\n",
      "vincent          :   9.063156965830961\n",
      "maggie           :   9.066293520456885\n",
      "hamilton         :   9.069684751450241\n",
      "hudson           :   9.103670036152934\n",
      "bacon            :   9.104474143165621\n"
     ]
    }
   ],
   "source": [
    "similar = analogy(layer0_w, ['elizabeth','he'], ['she'])\n",
    "for one in similar:\n",
    "    word = train_ds.get_ind_word(one[0]) \n",
    "    print(word,' '*(15-len(word)),':  ', one[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I think simon is a good choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}