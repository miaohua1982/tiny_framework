{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../datasets/qa1_single-supporting-fact_train.txt'\n",
    "f = open(path)\n",
    "raw = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noneed(r):\n",
    "    no_needs = ['0','1','2','3','4','5','6','7','8','9','\\n','\\t1', '\\t2','\\t3','\\t4','\\t5''\\t6','\\t7','\\t8','\\t9', '.','?','\\t']\n",
    "    for n in no_needs:\n",
    "        r = r.replace(n, '')\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_datasets = [remove_noneed(r).split(' ')[1:] for r in raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Mary', 'moved', 'to', 'the', 'bathroom'], ['John', 'went', 'to', 'the', 'hallway'], ['Where', 'is', 'Mary', 'bathroom'], ['Daniel', 'went', 'back', 'to', 'the', 'hallway'], ['Sandra', 'moved', 'to', 'the', 'garden'], ['Where', 'is', 'Daniel', 'hallway'], ['John', 'moved', 'to', 'the', 'office'], ['Sandra', 'journeyed', 'to', 'the', 'bathroom'], ['Where', 'is', 'Daniel', 'hallway'], ['Mary', 'moved', 'to', 'the', 'hallway']]\n",
      "[['Where', 'is', 'Sandra', 'bedroom'], ['Mary', 'journeyed', 'to', 'the', 'kitchen'], ['John', 'went', 'back', 'to', 'the', 'bedroom'], ['Where', 'is', 'Daniel', 'office'], ['Daniel', 'travelled', 'to', 'the', 'kitchen'], ['Sandra', 'travelled', 'to', 'the', 'kitchen'], ['Where', 'is', 'John', 'bedroom'], ['Sandra', 'travelled', 'to', 'the', 'hallway'], ['Daniel', 'went', 'to', 'the', 'garden'], ['Where', 'is', 'Daniel', 'garden']]\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "print(input_datasets[:10])\n",
    "print(input_datasets[-10:])\n",
    "print(len(input_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['1 Mary moved to the bathroom.\\n',\n '2 John went to the hallway.\\n',\n '3 Where is Mary? \\tbathroom\\t1\\n',\n '4 Daniel went back to the hallway.\\n',\n '5 Sandra moved to the garden.\\n',\n '6 Where is Daniel? \\thallway\\t4\\n',\n '7 John moved to the office.\\n',\n '8 Sandra journeyed to the bathroom.\\n',\n '9 Where is Daniel? \\thallway\\t4\\n',\n '10 Mary moved to the hallway.\\n']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4988/2802175153.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Dataset:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.start_token = '<START>'\n",
    "        self.word2ind = {}\n",
    "        self.ind2word = {}\n",
    "        self.corpus = []\n",
    "    \n",
    "    def remove_noneed(self, r):\n",
    "        no_needs = ['0','1','2','3','4','5','6','7','8','9','\\n','\\t1', '\\t2','\\t3','\\t4','\\t5''\\t6','\\t7','\\t8','\\t9', '.','?','\\t']\n",
    "        for n in no_needs:\n",
    "            r = r.replace(n, '')\n",
    "        return r\n",
    "\n",
    "    def parse(self):\n",
    "        f = open(path)\n",
    "        raw = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "        self.corpus = [remove_noneed(self, r).split(' ')[1:] for r in raw]\n",
    "        ind = 0\n",
    "        # add start token\n",
    "        self.word2ind[self.start_token] = ind\n",
    "        self.ind2word[ind] = self.start_token\n",
    "        ind = 1\n",
    "        \n",
    "        for sent in self.corpus:\n",
    "            for w in sent:\n",
    "                if w not in self.word2ind:\n",
    "                    self.word2ind[w] = ind\n",
    "                    self.ind2word[ind] = w\n",
    "                    ind += 1\n",
    "        np.random.shuffle(self.corpus)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return np.array([self.word2ind[self.start_token]]+[self.word2ind[w] for w in self.corpus[idx]])\n",
    "    \n",
    "    def get_sent(self, idx):\n",
    "        return self.corpus[idx]\n",
    "    \n",
    "    def get_vob_len(self):\n",
    "        return len(self.word2ind)\n",
    "    \n",
    "    def get_start_token(self):\n",
    "        return self.start_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Daniel', 'moved', 'to', 'the', 'bedroom'] ind: [ 0 11  2  3  4 18]\n",
      "we have 3000 training samples.\n"
     ]
    }
   ],
   "source": [
    "path = '../datasets/qa1_single-supporting-fact_train.txt'\n",
    "qa_ds = QA_Dataset(path)\n",
    "qa_ds.parse()\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(qa_ds, batch_size=batch_size, shuffle=True)  #20000\n",
    "print(qa_ds.get_sent(0), 'ind:', qa_ds[0])\n",
    "print('we have', len(qa_ds),'training samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(pred):\n",
    "    y = np.exp(pred).sum(axis=1, keepdims=True)\n",
    "    return np.exp(pred)/y\n",
    "\n",
    "def cross_entropy_loss(pred, gt):\n",
    "    num = gt.shape[0]\n",
    "    loss = -np.log(pred[np.arange(num), gt])\n",
    "    return np.sum(loss)\n",
    "\n",
    "# previous one may have problem, when pred = 0 or 1\n",
    "def cross_entropy_loss2(pred, gt):\n",
    "    loss = 0.0\n",
    "    num = gt.shape[0]\n",
    "    for one_pred, one_gt in zip(pred, gt):\n",
    "        cur_loss = 0\n",
    "        p = pred[one_gt]\n",
    "        if p == 0:\n",
    "            p += 1e+7\n",
    "            cur_loss = -1*np.log(p)\n",
    "        loss += cur_loss\n",
    "\n",
    "    return np.mean(loss)\n",
    "\n",
    "def calc_acc(pred, gt):\n",
    "    y = pred.argmax(axis=1)\n",
    "    return np.mean(y == gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have words 20\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "hidden_size = 10\n",
    "vocab = qa_ds.get_vob_len()\n",
    "\n",
    "word_embed = np.random.normal(0, 0.1, (vocab, hidden_size))-0.05\n",
    "transition_layer = np.eye(hidden_size)\n",
    "output_layer = np.random.normal(0,0.1, (hidden_size, vocab))-0.05\n",
    "h0 = np.zeros((1, hidden_size))\n",
    "print('we have words', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, h=None):\n",
    "    preds = []\n",
    "    hidden_state = []\n",
    "    \n",
    "    if h is None:   # h0\n",
    "        h = np.zeros((1,hidden_size))\n",
    "    hidden_state.append(h)\n",
    "    \n",
    "    for word in x:\n",
    "        word_vec = word_embed[[word]]\n",
    "        h = h.dot(transition_layer)+word_vec\n",
    "        pred = softmax(h.dot(output_layer))\n",
    "        \n",
    "        preds.append(pred)\n",
    "        hidden_state.append(h)\n",
    "        \n",
    "    return np.concatenate(preds), np.concatenate(hidden_state)\n",
    "\n",
    "def backward(preds, hidden_state, target):\n",
    "    num = preds.shape[0]\n",
    "    vocab_len = preds.shape[1]\n",
    "    hidden_size = hidden_state.shape[1]\n",
    "    \n",
    "    output_layer_delta = np.zeros((hidden_size, vocab_len))\n",
    "    transition_layer_delta = np.zeros((hidden_size, hidden_size))\n",
    "    word_embed_delta = np.zeros((num, hidden_size))\n",
    "    h0_delta = np.zeros((1, hidden_size))\n",
    "    \n",
    "    for ind in reversed(range(num)):\n",
    "        pred = preds[[ind]]\n",
    "        word = target[ind]\n",
    "        y = np.zeros((1,vocab_len))\n",
    "        prev_h = hidden_state[[ind]]   # to keep the hidden state's shape to 1*hidden_size, use [ind] not ind\n",
    "        cur_h = hidden_state[[ind+1]]  # to keep the hidden state's shape to 1*hidden_size, use [ind+1] not ind+1\n",
    "        y[0, word] = 1\n",
    "        delta = pred - y\n",
    "        output_layer_delta += cur_h.T.dot(delta)   # shape = hidden_size * vocab\n",
    "        \n",
    "        transition_layer_delta += prev_h.T.dot(delta.dot(output_layer.T))  # shape = hidden_size*hidden_size\n",
    "        word_embed_delta[[ind]] += delta.dot(output_layer.T)                    # shape = 1*hidden_size\n",
    "        \n",
    "        if ind>0:\n",
    "            # for previous layers' gradient\n",
    "            # because previous hidden state(h) & input word do contribute to current prediction\n",
    "            prev_gradient = delta.dot(output_layer.T)   # shape=1*hidden_size\n",
    "            for prev_ind in reversed(range(ind)):\n",
    "                prev_h = hidden_state[[prev_ind]]\n",
    "                cur_gradient = prev_gradient.dot(transition_layer.T)\n",
    "                transition_layer_delta += prev_h.T.dot(cur_gradient)              # shape = hidden_size*hidden_size\n",
    "                word_embed_delta[[prev_ind]] += cur_gradient                      # shape = 1*hidden_size\n",
    "                prev_gradient = cur_gradient\n",
    "        \n",
    "        h0_delta += prev_gradient.dot(transition_layer.T)\n",
    "        \n",
    "    return output_layer_delta, transition_layer_delta, word_embed_delta, h0_delta\n",
    "\n",
    "def step(output_layer_delta, transition_layer_delta, word_embed_delta, h0_delta, x):\n",
    "    global output_layer, transition_layer, word_embed, h0\n",
    "    num = x.shape[0]\n",
    "    output_layer -= alpha*output_layer_delta/num\n",
    "    transition_layer -= alpha*transition_layer_delta/num\n",
    "    word_embed[x] -= alpha*word_embed_delta/num\n",
    "    h0 -= h0_delta/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 6.951310559181079\n",
      "In epoch 0, train loss:10.9838, acc:0.3187\n",
      "Perplexity: 7.686951881924321\n",
      "In epoch 1, train loss:9.4104, acc:0.4002\n",
      "Perplexity: 12.413094807987672\n",
      "In epoch 2, train loss:9.1740, acc:0.4033\n",
      "Perplexity: 7.8934299815769515\n",
      "In epoch 3, train loss:9.0597, acc:0.4067\n",
      "Perplexity: 5.552090386425283\n",
      "In epoch 4, train loss:9.0259, acc:0.4097\n",
      "Perplexity: 9.631119372252218\n",
      "In epoch 5, train loss:8.8700, acc:0.4106\n",
      "Perplexity: 8.372487023931402\n",
      "In epoch 6, train loss:9.1166, acc:0.3641\n",
      "Perplexity: 3.819681996947627\n",
      "In epoch 7, train loss:8.4240, acc:0.4082\n",
      "Perplexity: 7.237313426697132\n",
      "In epoch 8, train loss:8.3644, acc:0.4174\n",
      "Perplexity: 3.6175414171901585\n",
      "In epoch 9, train loss:7.1224, acc:0.4539\n",
      "Perplexity: 3.627590472900009\n",
      "In epoch 10, train loss:6.2953, acc:0.4843\n",
      "Perplexity: 4.247551120181662\n",
      "In epoch 11, train loss:5.9678, acc:0.5017\n",
      "Perplexity: 2.906866548008806\n",
      "In epoch 12, train loss:5.7426, acc:0.5156\n",
      "Perplexity: 3.425728584616724\n",
      "In epoch 13, train loss:5.5821, acc:0.5211\n",
      "Perplexity: 3.255325634537227\n",
      "In epoch 14, train loss:5.4717, acc:0.5250\n",
      "Perplexity: 3.2682603484134245\n",
      "In epoch 15, train loss:5.3973, acc:0.5235\n",
      "Perplexity: 2.9081193985930356\n",
      "In epoch 16, train loss:5.3296, acc:0.5284\n",
      "Perplexity: 3.3634263942436817\n",
      "In epoch 17, train loss:5.2912, acc:0.5248\n",
      "Perplexity: 2.9364266760001287\n",
      "In epoch 18, train loss:5.2496, acc:0.5299\n",
      "Perplexity: 2.6051454371535114\n",
      "In epoch 19, train loss:5.2183, acc:0.5294\n"
     ]
    }
   ],
   "source": [
    "epoches = 20\n",
    "for epoch in range(epoches):\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    for x in train_dataloader:\n",
    "        sent = x.cpu().numpy()[0]\n",
    "    \n",
    "        x = sent[:-1]\n",
    "        y = sent[1:]\n",
    "    \n",
    "        preds, hidden_state = forward(x, h0)\n",
    "        cur_loss = cross_entropy_loss(preds, y)\n",
    "        loss += cur_loss\n",
    "        accuracy += calc_acc(preds, y)\n",
    "        output_layer_delta, transition_layer_delta, word_embed_delta, h0_delta = backward(preds, hidden_state, y)\n",
    "        step(output_layer_delta, transition_layer_delta, word_embed_delta, h0_delta, x)\n",
    "    \n",
    "    print(\"Perplexity:\", np.exp(cur_loss/len(y)))\n",
    "    print('In epoch %d, train loss:%.4f, acc:%.4f' % (epoch, loss/len(train_dataloader), accuracy/len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.75226863e-01,  5.82407737e-02,  5.66514107e-02,\n",
       "         7.81298138e-02,  2.36640783e-01,  1.22063891e-01,\n",
       "         2.03308131e-01, -9.55908078e-03,  9.99884053e-02,\n",
       "         2.17839597e-01],\n",
       "       [-4.32038916e-02,  7.68571418e-01, -1.19955967e-01,\n",
       "        -2.49706395e-02, -6.36187641e-02, -9.20372391e-02,\n",
       "         1.27189003e-01,  2.08306325e-01,  2.05126403e-01,\n",
       "        -5.00746898e-02],\n",
       "       [ 7.93179958e-02,  7.97998891e-02,  7.30468257e-01,\n",
       "         9.88133628e-02, -2.66347505e-02, -3.33391351e-02,\n",
       "        -2.10883846e-01, -3.87870576e-01, -4.03085882e-01,\n",
       "        -2.97779078e-02],\n",
       "       [ 8.87196829e-02, -1.27084121e-01,  1.92931664e-01,\n",
       "         7.34578959e-01,  7.29772396e-02, -1.44326803e-01,\n",
       "         2.14297255e-01,  4.21087971e-01,  3.42861821e-01,\n",
       "         1.60882723e-02],\n",
       "       [ 6.47908043e-02, -1.11352852e-01,  1.27028656e-01,\n",
       "        -2.48142272e-01,  8.47570100e-01, -2.16393152e-01,\n",
       "        -2.87956710e-04,  2.38027919e-01,  4.45441411e-02,\n",
       "        -6.95735847e-02],\n",
       "       [ 2.23863466e-01, -1.69636747e-01,  2.93913886e-01,\n",
       "        -2.86742207e-01, -1.66429598e-01,  1.43802658e-01,\n",
       "         6.51642451e-02,  4.73855650e-01, -1.37149938e-01,\n",
       "        -4.94375430e-01],\n",
       "       [-5.43949257e-01, -3.19632349e-01, -3.00740688e-01,\n",
       "        -2.43549900e-01, -5.12756357e-01,  5.00108157e-01,\n",
       "         6.72535447e-01,  8.96723468e-03,  1.75205760e-01,\n",
       "         4.27476338e-02],\n",
       "       [-2.84779672e-03, -6.89195839e-03,  5.49763250e-01,\n",
       "         4.90232899e-03,  3.79898144e-01,  1.93913541e-01,\n",
       "         4.84789084e-02,  7.07099965e-01, -1.43263027e-01,\n",
       "         8.38441796e-02],\n",
       "       [-3.92892079e-01, -1.90610095e-01,  2.88020882e-01,\n",
       "        -2.53357423e-01, -5.79185335e-02,  3.07708477e-01,\n",
       "        -7.75964552e-02, -6.58689607e-02,  9.02839527e-01,\n",
       "         3.55081874e-02],\n",
       "       [-8.31586849e-02, -2.91696303e-01,  1.76107733e-01,\n",
       "        -2.02577774e-01, -2.01162867e-01, -3.38263354e-01,\n",
       "         2.59486863e-02,  2.81286736e-01,  1.05475678e-01,\n",
       "         6.18793837e-01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_test(num=5):\n",
    "    for i in range(num):\n",
    "        sent_index = np.random.choice(len(qa_ds))\n",
    "        \n",
    "        preds,_ = forward(qa_ds[sent_index][:-1], h0)\n",
    "        print(qa_ds.get_sent(sent_index))\n",
    "        for pred, input_ind, target_ind in zip(preds,qa_ds[sent_index][:-1],qa_ds[sent_index][1:]):\n",
    "            input = qa_ds.ind2word[input_ind]\n",
    "            target = qa_ds.ind2word[target_ind]\n",
    "            pred = qa_ds.ind2word[pred.argmax()]\n",
    "            print(\"Prev Input:\", input, ' '*(20-len(input)), \"Target:\", target, ' '*(20-len(target)), \"Pred:\", pred)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'went', 'back', 'to', 'the', 'office']\n",
      "Prev Input: <START>               Target: John                  Pred: Where\n",
      "Prev Input: John                  Target: went                  Pred: went\n",
      "Prev Input: went                  Target: back                  Pred: to\n",
      "Prev Input: back                  Target: to                    Pred: to\n",
      "Prev Input: to                    Target: the                   Pred: the\n",
      "Prev Input: the                   Target: office                Pred: bathroom\n",
      "\n",
      "['John', 'went', 'to', 'the', 'bathroom']\n",
      "Prev Input: <START>               Target: John                  Pred: Where\n",
      "Prev Input: John                  Target: went                  Pred: went\n",
      "Prev Input: went                  Target: to                    Pred: to\n",
      "Prev Input: to                    Target: the                   Pred: the\n",
      "Prev Input: the                   Target: bathroom              Pred: garden\n",
      "\n",
      "['Daniel', 'journeyed', 'to', 'the', 'garden']\n",
      "Prev Input: <START>               Target: Daniel                Pred: Where\n",
      "Prev Input: Daniel                Target: journeyed             Pred: went\n",
      "Prev Input: journeyed             Target: to                    Pred: to\n",
      "Prev Input: to                    Target: the                   Pred: the\n",
      "Prev Input: the                   Target: garden                Pred: garden\n",
      "\n",
      "['Daniel', 'went', 'to', 'the', 'garden']\n",
      "Prev Input: <START>               Target: Daniel                Pred: Where\n",
      "Prev Input: Daniel                Target: went                  Pred: went\n",
      "Prev Input: went                  Target: to                    Pred: to\n",
      "Prev Input: to                    Target: the                   Pred: the\n",
      "Prev Input: the                   Target: garden                Pred: garden\n",
      "\n",
      "['Where', 'is', 'Daniel', 'hallway']\n",
      "Prev Input: <START>               Target: Where                 Pred: Where\n",
      "Prev Input: Where                 Target: is                    Pred: is\n",
      "Prev Input: is                    Target: Daniel                Pred: John\n",
      "Prev Input: Daniel                Target: hallway               Pred: office\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_for_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}