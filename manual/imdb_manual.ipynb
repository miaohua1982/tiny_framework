{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "reviews_path = '../reviews.txt'\n",
    "labels_path = '../labels.txt'\n",
    "\n",
    "class imdb_reviews:\n",
    "    def __init__(self, reviews_path, labels_path, train):\n",
    "        self.reviews_path = reviews_path\n",
    "        self.labels_path = labels_path\n",
    "        self.is_train = train\n",
    "        self.test_num = 1000\n",
    "        \n",
    "        self.reviews = []\n",
    "        self.labels = []\n",
    "        self.vocab = {}\n",
    "        self.vocab_reverse = {}\n",
    "        \n",
    "    def parse_reviews(self):\n",
    "        f = open(self.reviews_path)\n",
    "        raw_reviews = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "        f = open(self.labels_path)\n",
    "        raw_labels = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "        raw_reviews = [r.split() for r in raw_reviews]\n",
    "        ind = 0\n",
    "        for r in raw_reviews:\n",
    "            for w in r:\n",
    "                if w not in self.vocab:\n",
    "                    self.vocab[w] = ind\n",
    "                    self.vocab_reverse[ind] = w\n",
    "                    ind += 1\n",
    "                    \n",
    "        if self.is_train:\n",
    "            raw_reviews = raw_reviews[:-1*self.test_num]\n",
    "        else:\n",
    "            raw_reviews = raw_reviews[-1*self.test_num:]\n",
    "        \n",
    "        if self.is_train:\n",
    "            raw_labels = raw_labels[:-1*self.test_num]\n",
    "        else:\n",
    "            raw_labels = raw_labels[-1*self.test_num:]\n",
    "        raw_labels = [r[:-1] for r in raw_labels]\n",
    "        \n",
    "        self.reviews = raw_reviews\n",
    "        self.labels = raw_labels\n",
    "    \n",
    "    def get_vocab_len(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def get_word_ind(self, word):\n",
    "        return self.vocab[word]\n",
    "    \n",
    "    def get_ind_word(self, ind):\n",
    "        return self.vocab_reverse[ind]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        r = [self.vocab[w] for w in self.reviews[idx]]\n",
    "        r = np.array(list(set(r)))  # remove dups\n",
    "        l = 1 if self.labels[idx]=='positive' else 0\n",
    "        \n",
    "        return r, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 24000 train samples, and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "train_ds = imdb_reviews(reviews_path, labels_path, True)\n",
    "train_ds.parse_reviews()\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)  #20000\n",
    "\n",
    "test_ds = imdb_reviews(reviews_path, labels_path, False)\n",
    "test_ds.parse_reviews()\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)    #5000\n",
    "\n",
    "print('we have %d train samples, and %d test samples' % (len(train_dataloader), len(test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocab is 74073\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "vocab_len = train_ds.get_vocab_len()\n",
    "hidden_size = 128\n",
    "layer0_w = np.random.normal(0, 0.02, (vocab_len, hidden_size))-0.01\n",
    "layer1_w = np.random.normal(0, 0.02, (hidden_size, 1))-0.01\n",
    "print('length of vocab is', vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-1*x))\n",
    "\n",
    "def calc_acc(pred, gt):\n",
    "    y = (pred>=0.5).astype(np.int)\n",
    "    return np.mean(y==gt)\n",
    "\n",
    "def cross_entropy_loss(pred, gt):\n",
    "    loss = -gt*np.log(pred)-(1-gt)*np.log(1-pred)\n",
    "    return np.mean(loss)\n",
    "\n",
    "# previous one may have problem, when pred = 0 or 1\n",
    "def cross_entropy_loss2(pred, gt):\n",
    "    loss = 0.0\n",
    "    for one_pred, one_gt in zip(pred, gt):\n",
    "        cur_loss = 0\n",
    "        if one_gt == 1:\n",
    "            if one_pred == 0:\n",
    "                one_pred += 1e+7\n",
    "            cur_loss = -1*np.log(one_pred)\n",
    "        else:\n",
    "            if 1-one_pred == 0:\n",
    "                one_pred -= 1e-7\n",
    "            cur_loss = -1*np.log(1-one_pred)\n",
    "        loss += cur_loss\n",
    "\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    word_embed = []\n",
    "    for one in x:\n",
    "        w = np.mean(layer0_w[one], axis=0)\n",
    "        word_embed.append(w)\n",
    "    word_embed = np.stack(word_embed)\n",
    "\n",
    "    pred = word_embed.dot(layer1_w)\n",
    "    pred = sigmoid(pred)\n",
    "    \n",
    "    return pred, word_embed\n",
    "\n",
    "def backward(x, word_embed, pred, gt):\n",
    "    num = pred.shape[0]\n",
    "    delta = (pred-gt.reshape(num,1))/num\n",
    "    layer1_delta = word_embed.T.dot(delta)\n",
    "    \n",
    "    w_delta = delta.dot(layer1_w.T)\n",
    "    \n",
    "    return layer1_delta, w_delta\n",
    "\n",
    "def step(layer1_delta, w_delta, x):\n",
    "    global layer0_w, layer1_w\n",
    "    layer1_w = layer1_w - alpha*layer1_delta\n",
    "    \n",
    "    for ind, one in enumerate(x):\n",
    "        layer0_w[one] = layer0_w[one] - alpha*w_delta[ind]\n",
    "        \n",
    "def test():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for x,y in test_dataloader:\n",
    "        x = x.cpu().numpy()\n",
    "        y = y.cpu().numpy()\n",
    "        \n",
    "        pred, _ = forward(x)\n",
    "        loss += cross_entropy_loss2(pred, y)\n",
    "        accuracy += calc_acc(pred, y)\n",
    "    \n",
    "    return loss/len(test_dataloader), accuracy/len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in epoch 0, train loss: 0.3681, train acc: 0.8356, test loss: 0.3564, test acc: 0.8530\n",
      "in epoch 1, train loss: 0.2500, train acc: 0.9025, test loss: 0.3596, test acc: 0.8510\n",
      "in epoch 2, train loss: 0.2124, train acc: 0.9213, test loss: 0.3709, test acc: 0.8560\n",
      "in epoch 3, train loss: 0.1836, train acc: 0.9334, test loss: 0.4059, test acc: 0.8470\n",
      "in epoch 4, train loss: 0.1595, train acc: 0.9446, test loss: 0.4139, test acc: 0.8490\n",
      "in epoch 5, train loss: 0.1400, train acc: 0.9530, test loss: 0.4390, test acc: 0.8420\n",
      "in epoch 6, train loss: 0.1199, train acc: 0.9609, test loss: 0.4669, test acc: 0.8400\n",
      "in epoch 7, train loss: 0.1039, train acc: 0.9667, test loss: 0.4882, test acc: 0.8450\n",
      "in epoch 8, train loss: 0.0891, train acc: 0.9715, test loss: 0.6039, test acc: 0.8280\n",
      "in epoch 9, train loss: 0.0763, train acc: 0.9755, test loss: 0.5948, test acc: 0.8360\n",
      "in epoch 10, train loss: 0.0640, train acc: 0.9792, test loss: 0.6537, test acc: 0.8300\n",
      "in epoch 11, train loss: 0.0554, train acc: 0.9814, test loss: 0.7002, test acc: 0.8400\n",
      "in epoch 12, train loss: 0.0430, train acc: 0.9866, test loss: 0.8706, test acc: 0.8310\n",
      "in epoch 13, train loss: 0.0436, train acc: 0.9857, test loss: 0.8489, test acc: 0.8350\n",
      "in epoch 14, train loss: 0.0331, train acc: 0.9897, test loss: 0.8582, test acc: 0.8270\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for idx, (x,y) in enumerate(train_dataloader):\n",
    "        x = x.cpu().numpy()\n",
    "        y = y.cpu().numpy()\n",
    "        \n",
    "        pred, word_embed = forward(x)\n",
    "        loss += cross_entropy_loss2(pred, y)\n",
    "        accuracy += calc_acc(pred, y)\n",
    "        \n",
    "        layer1_delta, w_delta = backward(x, word_embed, pred, y)\n",
    "        step(layer1_delta, w_delta, x)\n",
    "    \n",
    "    # do test\n",
    "    test_loss, test_acc = test()\n",
    "    print('in epoch %d, train loss: %.4f, train acc: %.4f, test loss: %.4f, test acc: %.4f' % \\\n",
    "          (epoch, loss/(idx+1), accuracy/(idx+1), test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_similar(w_vec1, words_embed, topk=10):\n",
    "    similar_ratio = {}\n",
    "    for ind, one_vec in enumerate(words_embed):\n",
    "        s = np.sum((w_vec1-one_vec)**2)**0.5\n",
    "        similar_ratio[ind] = s\n",
    "    similar = sorted(similar_ratio.items(), key=lambda x:x[1])    \n",
    "    return similar[:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful ' similar top 10 is:\n",
      "beautiful : 0.0\n",
      "madness : 0.3410572914219501\n",
      "complaint : 0.37086161466842504\n",
      "cleverly : 0.38595111735481274\n",
      "episodes : 0.3860301084633669\n",
      "rapid : 0.38606041506936145\n",
      "deserves : 0.393883906669571\n",
      "enjoyed : 0.3973891232859879\n",
      "deanna : 0.41394740062047597\n",
      "bumped : 0.414539686895638\n"
     ]
    }
   ],
   "source": [
    "word = 'beautiful'\n",
    "word_ind = train_ds.get_word_ind(word)\n",
    "word_vec = layer0_w[word_ind,:]\n",
    "similar_topk = calc_similar(word_vec, layer0_w)\n",
    "print(word,'\\' similar top 10 is:')\n",
    "for one_similar in similar_topk:\n",
    "    print(train_ds.get_ind_word(one_similar[0]),':',one_similar[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrible ' similar top 10 is:\n",
      "terrible : 0.0\n",
      "horrible : 0.37675105548173093\n",
      "supposedly : 0.4239087393245637\n",
      "christmas : 0.45122855728386285\n",
      "confusing : 0.4881462609517464\n",
      "junk : 0.5213723338226643\n",
      "unintentional : 0.5332199904651174\n",
      "incoherent : 0.5621633789855115\n",
      "wayans : 0.568229124288795\n",
      "flimsy : 0.6304759367890448\n"
     ]
    }
   ],
   "source": [
    "word = 'terrible'\n",
    "word_ind = train_ds.get_word_ind(word)\n",
    "word_vec = layer0_w[word_ind,:]\n",
    "similar_topk = calc_similar(word_vec, layer0_w)\n",
    "print(word,'\\' similar top 10 is:')\n",
    "for one_similar in similar_topk:\n",
    "    print(train_ds.get_ind_word(one_similar[0]),':',one_similar[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
