{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batchnorm_forward(x, gamma, beta, eps):\n",
    " \n",
    "  N, C, H, W = x.shape\n",
    "  #step1: calculate mean\n",
    "  mu = 1./N/H/W * np.sum(x, axis = (0,2,3), keepdims=True)\n",
    "  #step2: subtract mean vector of every trainings example\n",
    "  xmu = x - mu\n",
    "  #step3: following the lower branch - calculation denominator\n",
    "  sq = xmu ** 2\n",
    "  #step4: calculate variance\n",
    "  var = 1./N/H/W * np.sum(sq, axis = (0,2,3), keepdims=True)\n",
    "  #step5: add eps for numerical stability, then sqrt\n",
    "  sqrtvar = np.sqrt(var + eps)\n",
    "  #step6: invert sqrtwar\n",
    "  ivar = 1./sqrtvar\n",
    "  #step7: execute normalization\n",
    "  xhat = xmu * ivar\n",
    "  #step8: Nor the two transformation steps\n",
    "  gammax = gamma * xhat\n",
    "  #step9\n",
    "  out = gammax + beta\n",
    "  #store intermediate\n",
    "  cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
    " \n",
    "  return out, cache\n",
    "\n",
    "def batchnorm_backward(dout, cache):\n",
    " \n",
    "  #unfold the variables stored in cache\n",
    "  xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\n",
    " \n",
    "  #get the dimensions of the input/output\n",
    "  N,C,H,W = dout.shape\n",
    " \n",
    "  #step9\n",
    "  dbeta = np.sum(dout, axis=(0,2,3))\n",
    "  dgammax = dout #not necessary, but more understandable\n",
    " \n",
    "  #step8\n",
    "  dgamma = np.sum(dgammax*xhat, axis=(0,2,3))\n",
    "  dxhat = dgammax * gamma\n",
    " \n",
    "  #step7\n",
    "  divar = np.sum(dxhat*xmu, axis=(0,2,3), keepdims=True)\n",
    "  dxmu1 = dxhat * ivar\n",
    " \n",
    "  #step6\n",
    "  dsqrtvar = -1. /(sqrtvar**2) * divar   # 1,c,1,1\n",
    " \n",
    "  #step5\n",
    "  dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar  #1,c,1,1\n",
    " \n",
    "  #step4\n",
    "  dsq = 1. /N/H/W * np.ones((N,C,H,W)) * dvar\n",
    " \n",
    "  #step3\n",
    "  dxmu2 = 2 * xmu * dsq\n",
    " \n",
    "  #step2\n",
    "  dx1 = (dxmu1 + dxmu2)\n",
    "  dmu = -1 * np.sum(dxmu1+dxmu2, axis=(0,2,3), keepdims=True)\n",
    " \n",
    "  #step1\n",
    "  dx2 = 1. /N/H/W * np.ones((N,C,H,W)) * dmu\n",
    " \n",
    "  #step0\n",
    "  dx = dx1 + dx2\n",
    " \n",
    "  return dx, dgamma, dbeta\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1.0382 -0.0181]\n",
      "   [-0.7247  0.7601]]\n",
      "\n",
      "  [[ 0.2904  1.237 ]\n",
      "   [ 0.6701 -0.1244]]]\n",
      "\n",
      "\n",
      " [[[-0.7049 -1.0384]\n",
      "   [-1.0714  1.7593]]\n",
      "\n",
      "  [[-1.4978  0.8259]\n",
      "   [ 0.3025 -1.7037]]]]\n",
      "tensor([[[[ 1.0382, -0.0181],\n",
      "          [-0.7247,  0.7601]],\n",
      "\n",
      "         [[ 0.2904,  1.2370],\n",
      "          [ 0.6701, -0.1244]]],\n",
      "\n",
      "\n",
      "        [[[-0.7049, -1.0384],\n",
      "          [-1.0714,  1.7593]],\n",
      "\n",
      "         [[-1.4978,  0.8259],\n",
      "          [ 0.3025, -1.7037]]]], grad_fn=<NativeBatchNormBackward>)\n",
      "[[[[ 4.4408921e-16  0.0000000e+00]\n",
      "   [ 0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      "  [[ 0.0000000e+00  0.0000000e+00]\n",
      "   [ 0.0000000e+00  0.0000000e+00]]]\n",
      "\n",
      "\n",
      " [[[ 0.0000000e+00 -4.4408921e-16]\n",
      "   [-4.4408921e-16  4.4408921e-16]]\n",
      "\n",
      "  [[ 0.0000000e+00  0.0000000e+00]\n",
      "   [ 0.0000000e+00  0.0000000e+00]]]]\n",
      "[-8.8817842e-16  0.0000000e+00]\n",
      "[8. 8.]\n",
      "tensor([[[[ 5.4929e-08, -9.5836e-10],\n",
      "          [-3.8344e-08,  4.0215e-08]],\n",
      "\n",
      "         [[-7.7724e-08, -3.3107e-07],\n",
      "          [-1.7934e-07,  3.3305e-08]]],\n",
      "\n",
      "\n",
      "        [[[-3.7295e-08, -5.4942e-08],\n",
      "          [-5.6691e-08,  9.3086e-08]],\n",
      "\n",
      "         [[ 4.0087e-07, -2.2105e-07],\n",
      "          [-8.0951e-08,  4.5596e-07]]]])\n",
      "tensor(1.)\n",
      "tensor([[[[1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.]]]])\n",
      "tensor([-1.3756e-07,  5.0521e-07])\n",
      "tensor([8., 8.])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "x = np.random.rand(2,2,2,2)\n",
    "gamma = np.array([1,1])\n",
    "beta = np.array([0,0])\n",
    "eps = 1e-5\n",
    "output, cache = batchnorm_forward(x, gamma, beta, eps)\n",
    "\n",
    "x_t = t.tensor(x.tolist(), requires_grad=True)\n",
    "bn2d = t.nn.BatchNorm2d(2)\n",
    "output_t = bn2d(x_t)\n",
    "\n",
    "print(output.round(4))\n",
    "print(output_t)\n",
    "\n",
    "dout = np.ones(x.shape)\n",
    "dx,dgamma,dbeta = batchnorm_backward(dout, cache)\n",
    "print(dx)\n",
    "print(dgamma)\n",
    "print(dbeta)\n",
    "\n",
    "\n",
    "bn2d = t.nn.BatchNorm2d(2)\n",
    "output_t = bn2d(x_t)\n",
    "f_t = output_t.sum()\n",
    "f_t.retain_grad()\n",
    "output_t.retain_grad()\n",
    "x_t.retain_grad()\n",
    "f_t.backward()\n",
    "print(x_t.grad)\n",
    "print(f_t.grad)\n",
    "print(output_t.grad)\n",
    "print(bn2d.weight.grad)\n",
    "print(bn2d.bias.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1.0382 -0.0181]\n",
      "   [-0.7247  0.7601]]\n",
      "\n",
      "  [[ 0.2904  1.237 ]\n",
      "   [ 0.6701 -0.1244]]]\n",
      "\n",
      "\n",
      " [[[-0.7049 -1.0384]\n",
      "   [-1.0714  1.7593]]\n",
      "\n",
      "  [[-1.4978  0.8259]\n",
      "   [ 0.3025 -1.7037]]]]\n"
     ]
    }
   ],
   "source": [
    "# Forward pass \n",
    "N,C,H,W = x.shape\n",
    "mu = 1.0/N/H/W*np.sum(x,axis = (0,2,3),keepdims=True) # Size (H,) \n",
    "sigma2 = 1/N/H/W*np.sum((x-mu)**2,axis=(0,2,3), keepdims=True)# Size (H,) \n",
    "hath = (x-mu)*(sigma2+eps)**(-1./2.)\n",
    "y = gamma*hath+beta\n",
    "print(y.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 2.72836982e-16 -4.76023829e-18]\n",
      "   [-1.90456787e-16  1.99748982e-16]]\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[-1.85244992e-16 -2.72901185e-16]\n",
      "   [-2.81584924e-16  4.62362163e-16]]\n",
      "\n",
      "  [[ 0.00000000e+00  0.00000000e+00]\n",
      "   [ 0.00000000e+00  0.00000000e+00]]]]\n",
      "[-8.8817842e-16  0.0000000e+00]\n",
      "[8. 8.]\n"
     ]
    }
   ],
   "source": [
    "dy = np.ones(x.shape)\n",
    "mu = 1.0/N/H/W*np.sum(x, axis = (0,2,3), keepdims=True) # Size (H,) \n",
    "var = 1.0/N/H/W*np.sum((x-mu)**2, axis=(0,2,3), keepdims=True)# Size (H,) \n",
    "dbeta = np.sum(dy, axis=(0,2,3))\n",
    "dgamma = np.sum((x - mu) * (var + eps)**(-1. / 2.) * dy, axis=(0,2,3))\n",
    "dx = (1./ N/H/W) * gamma * (var + eps)**(-1. / 2.) * (N*H*W *dy - np.sum(dy, axis=(0,2,3), keepdims=True)\n",
    "    - (x - mu) * (var + eps)**(-1.0) * np.sum(dy * (x - mu), axis=(0,2,3), keepdims=True))\n",
    "print(dx)\n",
    "print(dgamma)\n",
    "print(dbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-37-4a406111c088>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-4a406111c088>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [[[[ 4.4408921e-16  0.0000000e+00]\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[[[[ 4.4408921e-16  0.0000000e+00]\n",
    "   [ 0.0000000e+00  0.0000000e+00]]\n",
    "\n",
    "  [[ 0.0000000e+00  0.0000000e+00]\n",
    "   [ 0.0000000e+00  0.0000000e+00]]]\n",
    "\n",
    "\n",
    " [[[ 0.0000000e+00 -4.4408921e-16]\n",
    "   [-4.4408921e-16  4.4408921e-16]]\n",
    "\n",
    "  [[ 0.0000000e+00  0.0000000e+00]\n",
    "   [ 0.0000000e+00  0.0000000e+00]]]]\n",
    "[-8.8817842e-16  0.0000000e+00]\n",
    "[8. 8.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
